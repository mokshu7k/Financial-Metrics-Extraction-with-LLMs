2025-10-05 17:52:36,382 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 17:52:36,382 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 17:52:36,382 - __main__ - INFO - ================================================================================
2025-10-05 17:52:36,382 - __main__ - INFO - STEP 1: DATA PREPROCESSING
2025-10-05 17:52:36,382 - __main__ - INFO - ================================================================================
2025-10-05 17:52:36,417 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:06:58,633 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:06:58,633 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:06:58,633 - __main__ - INFO - ================================================================================
2025-10-05 18:06:58,633 - __main__ - INFO - STEP 2: DOCUMENT CHUNKING
2025-10-05 18:06:58,633 - __main__ - INFO - ================================================================================
2025-10-05 18:06:58,655 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:06:59,071 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,071 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2024.json: 37 chunks
2025-10-05 18:06:59,094 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,094 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2025.json: 28 chunks
2025-10-05 18:06:59,116 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,116 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2024.json: 49 chunks
2025-10-05 18:06:59,250 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,250 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2025.json: 43 chunks
2025-10-05 18:06:59,274 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,274 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2024.json: 42 chunks
2025-10-05 18:06:59,293 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,293 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2025.json: 31 chunks
2025-10-05 18:06:59,401 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,401 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2024.json: 69 chunks
2025-10-05 18:06:59,451 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2025-10-05 18:06:59,451 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2025.json: 56 chunks
2025-10-05 18:06:59,491 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,491 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2024.json: 31 chunks
2025-10-05 18:06:59,507 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,508 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2025.json: 31 chunks
2025-10-05 18:06:59,532 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,533 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2024.json: 43 chunks
2025-10-05 18:06:59,550 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,551 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2025.json: 44 chunks
2025-10-05 18:06:59,566 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,567 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2024.json: 34 chunks
2025-10-05 18:06:59,583 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,584 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2025.json: 35 chunks
2025-10-05 18:06:59,590 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,599 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2024.json: 29 chunks
2025-10-05 18:06:59,619 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,620 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2025.json: 55 chunks
2025-10-05 18:06:59,652 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,652 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q1_2024.json: 19 chunks
2025-10-05 18:06:59,666 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,667 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q1_2025.json: 16 chunks
2025-10-05 18:06:59,683 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,684 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q2_2024.json: 35 chunks
2025-10-05 18:06:59,719 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:06:59,719 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q2_2025.json: 34 chunks
2025-10-05 18:06:59,755 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,755 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q3_2024.json: 24 chunks
2025-10-05 18:06:59,774 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,774 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q3_2025.json: 24 chunks
2025-10-05 18:06:59,793 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,793 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q4_2024.json: 49 chunks
2025-10-05 18:06:59,812 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,813 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q4_2025.json: 57 chunks
2025-10-05 18:06:59,887 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,888 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q1_2024.json: 3 chunks
2025-10-05 18:06:59,898 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,898 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q1_2025.json: 11 chunks
2025-10-05 18:06:59,911 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,911 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q2_2024.json: 2 chunks
2025-10-05 18:06:59,976 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,977 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q2_2025.json: 43 chunks
2025-10-05 18:06:59,990 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:06:59,990 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q3_2024.json: 16 chunks
2025-10-05 18:07:00,034 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,034 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q3_2025.json: 2 chunks
2025-10-05 18:07:00,056 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,056 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q4_2024.json: 12 chunks
2025-10-05 18:07:00,074 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,074 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q4_2025.json: 67 chunks
2025-10-05 18:07:00,258 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,273 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2024.json: 46 chunks
2025-10-05 18:07:00,337 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,337 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2025.json: 50 chunks
2025-10-05 18:07:00,420 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,421 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2024.json: 47 chunks
2025-10-05 18:07:00,528 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,528 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2025.json: 46 chunks
2025-10-05 18:07:00,555 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,555 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2024.json: 50 chunks
2025-10-05 18:07:00,571 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,571 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2025.json: 46 chunks
2025-10-05 18:07:00,655 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,656 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2024.json: 46 chunks
2025-10-05 18:07:00,750 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,750 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2025.json: 48 chunks
2025-10-05 18:07:00,817 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2025-10-05 18:07:00,817 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2024.json: 62 chunks
2025-10-05 18:07:00,835 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2025-10-05 18:07:00,836 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2025.json: 59 chunks
2025-10-05 18:07:00,908 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,909 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2024.json: 38 chunks
2025-10-05 18:07:00,926 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,927 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2025.json: 69 chunks
2025-10-05 18:07:00,939 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,939 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2024.json: 52 chunks
2025-10-05 18:07:00,963 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:00,964 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2025.json: 63 chunks
2025-10-05 18:07:00,982 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:07:00,983 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2024.json: 75 chunks
2025-10-05 18:07:01,211 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,211 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2025.json: 47 chunks
2025-10-05 18:07:01,227 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,227 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q1_2024.json: 53 chunks
2025-10-05 18:07:01,284 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,284 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q2_2024.json: 48 chunks
2025-10-05 18:07:01,326 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,327 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q2_2025.json: 61 chunks
2025-10-05 18:07:01,366 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,367 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q3_2024.json: 59 chunks
2025-10-05 18:07:01,367 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,367 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q3_2025.json: 26 chunks
2025-10-05 18:07:01,387 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,399 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q4_2024.json: 55 chunks
2025-10-05 18:07:01,418 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:07:01,419 - src.data_preprocessing.chunking - INFO - Chunked MARICO_Q4_2025.json: 60 chunks
2025-10-05 18:07:01,437 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:07:01,438 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q1_2024.json: 58 chunks
2025-10-05 18:07:01,486 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,487 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q1_2025.json: 47 chunks
2025-10-05 18:07:01,507 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:07:01,534 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q2_2024.json: 56 chunks
2025-10-05 18:07:01,563 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2025-10-05 18:07:01,563 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q2_2025.json: 55 chunks
2025-10-05 18:07:01,607 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,607 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q3_2024.json: 55 chunks
2025-10-05 18:07:01,624 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,624 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q3_2025.json: 52 chunks
2025-10-05 18:07:01,640 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,641 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q4_2024.json: 44 chunks
2025-10-05 18:07:01,658 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2025-10-05 18:07:01,658 - src.data_preprocessing.chunking - INFO - Chunked TATACONSUM_Q4_2025.json: 50 chunks
2025-10-05 18:07:01,658 - src.utils.helpers - INFO - chunk_all_documents took 3.00 seconds
2025-10-05 18:07:01,700 - src.data_preprocessing.chunking - INFO - Saved 1071 financial chunks to financial_chunks_fixed_size.json
2025-10-05 18:07:01,773 - src.data_preprocessing.chunking - INFO - Saved 1623 transcript chunks to transcript_chunks_fixed_size.json
2025-10-05 18:07:01,836 - experiment.financial_rag_pipeline - INFO - Metrics:
2025-10-05 18:07:01,836 - experiment.financial_rag_pipeline - INFO -   chunking_strategy: fixed_size
2025-10-05 18:07:01,837 - experiment.financial_rag_pipeline - INFO -   chunking_total_chunks: 2694
2025-10-05 18:07:01,837 - experiment.financial_rag_pipeline - INFO -   chunking_financial: 1071
2025-10-05 18:07:01,838 - experiment.financial_rag_pipeline - INFO -   chunking_transcript: 1623
2025-10-05 18:07:01,839 - src.utils.helpers - INFO - run_chunking took 3.21 seconds
2025-10-05 18:07:30,696 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:07:30,696 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:07:30,696 - __main__ - INFO - ================================================================================
2025-10-05 18:07:30,696 - __main__ - INFO - STEP 3: EMBEDDING GENERATION & VECTOR STORE
2025-10-05 18:07:30,696 - __main__ - INFO - ================================================================================
2025-10-05 18:07:30,724 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:07:30,744 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:07:30,747 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-05 18:07:34,540 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-10-05 18:07:59,867 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-05 18:07:59,867 - src.utils.helpers - INFO - _initialize_model took 29.12 seconds
2025-10-05 18:07:59,898 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1071 chunks from financial_chunks_fixed_size.json
2025-10-05 18:08:46,758 - src.utils.helpers - INFO - generate_embeddings took 46.86 seconds
2025-10-05 18:08:46,792 - src.utils.helpers - INFO - embed_chunks_from_file took 46.92 seconds
2025-10-05 18:08:46,792 - src.data_preprocessing.embed_store - INFO - Processed 1071 financial chunks
2025-10-05 18:08:46,836 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1623 chunks from transcript_chunks_fixed_size.json
2025-10-05 18:09:58,824 - src.utils.helpers - INFO - generate_embeddings took 71.99 seconds
2025-10-05 18:09:58,874 - src.utils.helpers - INFO - embed_chunks_from_file took 72.08 seconds
2025-10-05 18:09:58,874 - src.data_preprocessing.embed_store - INFO - Processed 1623 transcript chunks
2025-10-05 18:09:58,954 - src.utils.helpers - INFO - add_embeddings took 0.03 seconds
2025-10-05 18:09:58,955 - src.utils.helpers - INFO - create_vector_store_from_chunks took 119.08 seconds
2025-10-05 18:09:58,955 - src.utils.helpers - INFO - run_embedding_generation took 148.26 seconds
2025-10-05 18:09:58,955 - __main__ - ERROR - 
Pipeline failed with error: in method 'fvec_renorm_L2', argument 3 of type 'float *'
Traceback (most recent call last):
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 699, in main
    pipeline.run_embedding_generation(force=args.force_reembed)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\utils\helpers.py", line 45, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 259, in run_embedding_generation
    num_chunks = self.vector_manager.create_vector_store_from_chunks(strategy)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\utils\helpers.py", line 45, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\data_preprocessing\embed_store.py", line 529, in create_vector_store_from_chunks
    self.vector_store.add_embeddings(embeddings, all_chunks)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\utils\helpers.py", line 45, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\data_preprocessing\embed_store.py", line 232, in add_embeddings
    faiss.normalize_L2(embeddings)
  File "C:\Users\lenovo\anaconda3\envs\financial-rag\lib\site-packages\faiss\extra_wrappers.py", line 143, in normalize_L2
    fvec_renorm_L2(x.shape[1], x.shape[0], swig_ptr(x))
  File "C:\Users\lenovo\anaconda3\envs\financial-rag\lib\site-packages\faiss\swigfaiss_avx2.py", line 1414, in fvec_renorm_L2
    return _swigfaiss_avx2.fvec_renorm_L2(d, nx, x)
TypeError: in method 'fvec_renorm_L2', argument 3 of type 'float *'
2025-10-05 18:13:29,201 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:13:29,201 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:13:29,201 - __main__ - INFO - ================================================================================
2025-10-05 18:13:29,201 - __main__ - INFO - STEP 3: EMBEDDING GENERATION & VECTOR STORE
2025-10-05 18:13:29,201 - __main__ - INFO - ================================================================================
2025-10-05 18:13:29,241 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:13:29,269 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:13:29,273 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-05 18:13:32,702 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-05 18:13:32,702 - src.utils.helpers - INFO - _initialize_model took 3.43 seconds
2025-10-05 18:13:32,718 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1071 chunks from financial_chunks_fixed_size.json
2025-10-05 18:14:29,853 - src.utils.helpers - INFO - generate_embeddings took 57.13 seconds
2025-10-05 18:14:29,876 - src.utils.helpers - INFO - embed_chunks_from_file took 57.17 seconds
2025-10-05 18:14:29,876 - src.data_preprocessing.embed_store - INFO - Processed 1071 financial chunks
2025-10-05 18:14:29,917 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1623 chunks from transcript_chunks_fixed_size.json
2025-10-05 18:15:49,230 - src.utils.helpers - INFO - generate_embeddings took 79.31 seconds
2025-10-05 18:15:49,279 - src.utils.helpers - INFO - embed_chunks_from_file took 79.40 seconds
2025-10-05 18:15:49,280 - src.data_preprocessing.embed_store - INFO - Processed 1623 transcript chunks
2025-10-05 18:15:49,419 - src.data_preprocessing.embed_store - INFO - Added 2694 embeddings to FAISS index
2025-10-05 18:15:49,420 - src.utils.helpers - INFO - add_embeddings took 0.11 seconds
2025-10-05 18:15:49,423 - src.data_preprocessing.embed_store - INFO - Saved FAISS index to data\embeddings\financial_index.faiss
2025-10-05 18:15:49,463 - src.data_preprocessing.embed_store - INFO - Created vector store with 2694 chunks
2025-10-05 18:15:49,476 - src.utils.helpers - INFO - create_vector_store_from_chunks took 136.77 seconds
2025-10-05 18:15:49,480 - experiment.financial_rag_pipeline - INFO - Metrics:
2025-10-05 18:15:49,480 - experiment.financial_rag_pipeline - INFO -   embedding_model: all-MiniLM-L6-v2
2025-10-05 18:15:49,481 - experiment.financial_rag_pipeline - INFO -   embedding_dimension: 384
2025-10-05 18:15:49,482 - experiment.financial_rag_pipeline - INFO -   embedding_num_chunks: 2694
2025-10-05 18:15:49,482 - src.utils.helpers - INFO - run_embedding_generation took 140.28 seconds
2025-10-05 18:16:09,837 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:16:09,837 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:16:09,837 - __main__ - WARNING - GROQ_API_KEY not found in environment
2025-10-05 18:16:09,944 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:16:09,944 - __main__ - INFO - ================================================================================
2025-10-05 18:16:09,944 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:16:09,944 - __main__ - INFO - ================================================================================
2025-10-05 18:16:09,944 - __main__ - ERROR - Groq API client not available for model: gpt-3.5-turbo
2025-10-05 18:16:09,944 - src.utils.helpers - INFO - run_zero_shot_extraction took 0.00 seconds
2025-10-05 18:17:00,033 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:17:00,034 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:17:00,035 - __main__ - WARNING - GROQ_API_KEY not found in environment
2025-10-05 18:17:00,089 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:17:00,089 - __main__ - INFO - ================================================================================
2025-10-05 18:17:00,089 - __main__ - INFO - STEP 4b: CHAIN-OF-THOUGHT EXTRACTION
2025-10-05 18:17:00,089 - __main__ - INFO - ================================================================================
2025-10-05 18:17:00,089 - __main__ - ERROR - Groq API client not available for model: gpt-4
2025-10-05 18:17:00,089 - src.utils.helpers - INFO - run_cot_extraction took 0.00 seconds
2025-10-05 18:17:14,967 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:17:14,968 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:17:14,969 - __main__ - WARNING - GROQ_API_KEY not found in environment
2025-10-05 18:17:15,021 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:17:15,021 - __main__ - INFO - ================================================================================
2025-10-05 18:17:15,021 - __main__ - INFO - STEP 4c: RAG EXTRACTION
2025-10-05 18:17:15,021 - __main__ - INFO - ================================================================================
2025-10-05 18:17:15,021 - __main__ - ERROR - Vector store not initialized. Run embedding generation first.
2025-10-05 18:17:15,021 - src.utils.helpers - INFO - run_rag_extraction took 0.00 seconds
2025-10-05 18:18:03,740 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:18:03,741 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:18:03,741 - __main__ - INFO - ================================================================================
2025-10-05 18:18:03,742 - __main__ - INFO - STEP 5: EVALUATION & COMPARISON
2025-10-05 18:18:03,742 - __main__ - INFO - ================================================================================
2025-10-05 18:18:03,765 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:18:03,767 - experiment.unified_evaluation - INFO - Started experiment: unified_evaluation
2025-10-05 18:18:03,768 - src.evaluation.evaluator - ERROR - No methods found to evaluate
2025-10-05 18:18:03,769 - src.utils.helpers - INFO - evaluate_all_methods took 0.00 seconds
2025-10-05 18:18:03,774 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251005_181803.json
2025-10-05 18:18:03,784 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251005_181803.csv
2025-10-05 18:18:03,785 - src.utils.helpers - INFO - run_evaluation took 0.04 seconds
2025-10-05 18:18:03,785 - __main__ - ERROR - 
Pipeline failed with error: ' font-family'
Traceback (most recent call last):
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 721, in main
    pipeline.run_evaluation()
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\utils\helpers.py", line 45, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 493, in run_evaluation
    html_report = self.evaluator.generate_evaluation_report(
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\evaluation\evaluator.py", line 416, in generate_evaluation_report
    html_content = self._generate_html_report(comparison_results)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\evaluation\evaluator.py", line 455, in _generate_html_report
    html = """
KeyError: ' font-family'
2025-10-05 18:24:53,662 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:24:53,663 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:24:53,664 - __main__ - INFO - ================================================================================
2025-10-05 18:24:53,664 - __main__ - INFO - STEP 5: EVALUATION & COMPARISON
2025-10-05 18:24:53,664 - __main__ - INFO - ================================================================================
2025-10-05 18:24:53,687 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-05 18:24:53,689 - experiment.unified_evaluation - INFO - Started experiment: unified_evaluation
2025-10-05 18:24:53,690 - src.evaluation.evaluator - ERROR - No methods found to evaluate
2025-10-05 18:24:53,691 - src.utils.helpers - INFO - evaluate_all_methods took 0.00 seconds
2025-10-05 18:24:53,696 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251005_182453.json
2025-10-05 18:24:53,699 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251005_182453.csv
2025-10-05 18:24:53,699 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251005_182453.html
2025-10-05 18:24:53,699 - __main__ - INFO - 
Evaluation reports generated:
2025-10-05 18:24:53,699 - __main__ - INFO -   - JSON: results\evaluation_reports\evaluation_report_20251005_182453.json
2025-10-05 18:24:53,699 - __main__ - INFO -   - CSV: results\evaluation_reports\evaluation_report_20251005_182453.csv
2025-10-05 18:24:53,699 - __main__ - INFO -   - HTML: results\evaluation_reports\evaluation_report_20251005_182453.html
2025-10-05 18:24:53,699 - src.utils.helpers - INFO - run_evaluation took 0.03 seconds
2025-10-05 18:28:08,084 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:28:08,084 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:28:08,084 - __main__ - WARNING - GROQ_API_KEY not found in environment
2025-10-05 18:28:08,185 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:28:08,185 - __main__ - INFO - ================================================================================
2025-10-05 18:28:08,185 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:28:08,185 - __main__ - INFO - ================================================================================
2025-10-05 18:28:08,185 - __main__ - ERROR - Groq API client not available for model: gpt-3.5-turbo
2025-10-05 18:28:08,200 - src.utils.helpers - INFO - run_zero_shot_extraction took 0.01 seconds
2025-10-05 18:30:45,383 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:30:45,398 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:30:45,769 - __main__ - INFO - Groq client initialized
2025-10-05 18:30:45,824 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:30:45,824 - __main__ - INFO - ================================================================================
2025-10-05 18:30:45,825 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:30:45,826 - __main__ - INFO - ================================================================================
2025-10-05 18:30:45,827 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:30:45,827 - src.methods.zero_shot - INFO - Processing document 1/63: Unknown
2025-10-05 18:30:45,828 - src.methods.zero_shot - ERROR - Failed to process document 1: 'company'
2025-10-05 18:30:45,828 - src.methods.zero_shot - INFO - Processing document 2/63: Unknown
2025-10-05 18:30:45,829 - src.methods.zero_shot - ERROR - Failed to process document 2: 'company'
2025-10-05 18:30:45,829 - src.methods.zero_shot - INFO - Processing document 3/63: Unknown
2025-10-05 18:30:45,830 - src.methods.zero_shot - ERROR - Failed to process document 3: 'company'
2025-10-05 18:30:45,830 - src.methods.zero_shot - INFO - Processing document 4/63: Unknown
2025-10-05 18:30:45,830 - src.methods.zero_shot - ERROR - Failed to process document 4: 'company'
2025-10-05 18:30:45,831 - src.methods.zero_shot - INFO - Processing document 5/63: Unknown
2025-10-05 18:30:45,831 - src.methods.zero_shot - ERROR - Failed to process document 5: 'company'
2025-10-05 18:30:45,832 - src.methods.zero_shot - INFO - Processing document 6/63: Unknown
2025-10-05 18:30:45,832 - src.methods.zero_shot - ERROR - Failed to process document 6: 'company'
2025-10-05 18:30:45,833 - src.methods.zero_shot - INFO - Processing document 7/63: Unknown
2025-10-05 18:30:45,834 - src.methods.zero_shot - ERROR - Failed to process document 7: 'company'
2025-10-05 18:30:45,834 - src.methods.zero_shot - INFO - Processing document 8/63: Unknown
2025-10-05 18:30:45,834 - src.methods.zero_shot - ERROR - Failed to process document 8: 'company'
2025-10-05 18:30:45,835 - src.methods.zero_shot - INFO - Processing document 9/63: Unknown
2025-10-05 18:30:45,835 - src.methods.zero_shot - ERROR - Failed to process document 9: 'company'
2025-10-05 18:30:45,836 - src.methods.zero_shot - INFO - Processing document 10/63: Unknown
2025-10-05 18:30:45,836 - src.methods.zero_shot - ERROR - Failed to process document 10: 'company'
2025-10-05 18:30:45,837 - src.methods.zero_shot - INFO - Processing document 11/63: Unknown
2025-10-05 18:30:45,837 - src.methods.zero_shot - ERROR - Failed to process document 11: 'company'
2025-10-05 18:30:45,838 - src.methods.zero_shot - INFO - Processing document 12/63: Unknown
2025-10-05 18:30:45,838 - src.methods.zero_shot - ERROR - Failed to process document 12: 'company'
2025-10-05 18:30:45,839 - src.methods.zero_shot - INFO - Processing document 13/63: Unknown
2025-10-05 18:30:45,839 - src.methods.zero_shot - ERROR - Failed to process document 13: 'company'
2025-10-05 18:30:45,839 - src.methods.zero_shot - INFO - Processing document 14/63: Unknown
2025-10-05 18:30:45,840 - src.methods.zero_shot - ERROR - Failed to process document 14: 'company'
2025-10-05 18:30:45,840 - src.methods.zero_shot - INFO - Processing document 15/63: Unknown
2025-10-05 18:30:45,840 - src.methods.zero_shot - ERROR - Failed to process document 15: 'company'
2025-10-05 18:30:45,841 - src.methods.zero_shot - INFO - Processing document 16/63: Unknown
2025-10-05 18:30:45,841 - src.methods.zero_shot - ERROR - Failed to process document 16: 'company'
2025-10-05 18:30:45,841 - src.methods.zero_shot - INFO - Processing document 17/63: Unknown
2025-10-05 18:30:45,842 - src.methods.zero_shot - ERROR - Failed to process document 17: 'company'
2025-10-05 18:30:45,842 - src.methods.zero_shot - INFO - Processing document 18/63: Unknown
2025-10-05 18:30:45,842 - src.methods.zero_shot - ERROR - Failed to process document 18: 'company'
2025-10-05 18:30:45,842 - src.methods.zero_shot - INFO - Processing document 19/63: Unknown
2025-10-05 18:30:45,843 - src.methods.zero_shot - ERROR - Failed to process document 19: 'company'
2025-10-05 18:30:45,843 - src.methods.zero_shot - INFO - Processing document 20/63: Unknown
2025-10-05 18:30:45,843 - src.methods.zero_shot - ERROR - Failed to process document 20: 'company'
2025-10-05 18:30:45,844 - src.methods.zero_shot - INFO - Processing document 21/63: Unknown
2025-10-05 18:30:45,844 - src.methods.zero_shot - ERROR - Failed to process document 21: 'company'
2025-10-05 18:30:45,844 - src.methods.zero_shot - INFO - Processing document 22/63: Unknown
2025-10-05 18:30:45,845 - src.methods.zero_shot - ERROR - Failed to process document 22: 'company'
2025-10-05 18:30:45,845 - src.methods.zero_shot - INFO - Processing document 23/63: Unknown
2025-10-05 18:30:45,845 - src.methods.zero_shot - ERROR - Failed to process document 23: 'company'
2025-10-05 18:30:45,846 - src.methods.zero_shot - INFO - Processing document 24/63: Unknown
2025-10-05 18:30:45,846 - src.methods.zero_shot - ERROR - Failed to process document 24: 'company'
2025-10-05 18:30:45,846 - src.methods.zero_shot - INFO - Processing document 25/63: Unknown
2025-10-05 18:30:45,846 - src.methods.zero_shot - ERROR - Failed to process document 25: 'company'
2025-10-05 18:30:45,847 - src.methods.zero_shot - INFO - Processing document 26/63: Unknown
2025-10-05 18:30:45,847 - src.methods.zero_shot - ERROR - Failed to process document 26: 'company'
2025-10-05 18:30:45,847 - src.methods.zero_shot - INFO - Processing document 27/63: Unknown
2025-10-05 18:30:45,848 - src.methods.zero_shot - ERROR - Failed to process document 27: 'company'
2025-10-05 18:30:45,848 - src.methods.zero_shot - INFO - Processing document 28/63: Unknown
2025-10-05 18:30:45,848 - src.methods.zero_shot - ERROR - Failed to process document 28: 'company'
2025-10-05 18:30:45,849 - src.methods.zero_shot - INFO - Processing document 29/63: Unknown
2025-10-05 18:30:45,849 - src.methods.zero_shot - ERROR - Failed to process document 29: 'company'
2025-10-05 18:30:45,849 - src.methods.zero_shot - INFO - Processing document 30/63: Unknown
2025-10-05 18:30:45,850 - src.methods.zero_shot - ERROR - Failed to process document 30: 'company'
2025-10-05 18:30:45,850 - src.methods.zero_shot - INFO - Processing document 31/63: Unknown
2025-10-05 18:30:45,851 - src.methods.zero_shot - ERROR - Failed to process document 31: 'company'
2025-10-05 18:30:45,851 - src.methods.zero_shot - INFO - Processing document 32/63: Unknown
2025-10-05 18:30:45,852 - src.methods.zero_shot - ERROR - Failed to process document 32: 'company'
2025-10-05 18:30:45,852 - src.methods.zero_shot - INFO - Processing document 33/63: Unknown
2025-10-05 18:30:45,853 - src.methods.zero_shot - ERROR - Failed to process document 33: 'company'
2025-10-05 18:30:45,853 - src.methods.zero_shot - INFO - Processing document 34/63: Unknown
2025-10-05 18:30:45,854 - src.methods.zero_shot - ERROR - Failed to process document 34: 'company'
2025-10-05 18:30:45,854 - src.methods.zero_shot - INFO - Processing document 35/63: Unknown
2025-10-05 18:30:45,855 - src.methods.zero_shot - ERROR - Failed to process document 35: 'company'
2025-10-05 18:30:45,855 - src.methods.zero_shot - INFO - Processing document 36/63: Unknown
2025-10-05 18:30:45,855 - src.methods.zero_shot - ERROR - Failed to process document 36: 'company'
2025-10-05 18:30:45,856 - src.methods.zero_shot - INFO - Processing document 37/63: Unknown
2025-10-05 18:30:45,856 - src.methods.zero_shot - ERROR - Failed to process document 37: 'company'
2025-10-05 18:30:45,857 - src.methods.zero_shot - INFO - Processing document 38/63: Unknown
2025-10-05 18:30:45,857 - src.methods.zero_shot - ERROR - Failed to process document 38: 'company'
2025-10-05 18:30:45,857 - src.methods.zero_shot - INFO - Processing document 39/63: Unknown
2025-10-05 18:30:45,857 - src.methods.zero_shot - ERROR - Failed to process document 39: 'company'
2025-10-05 18:30:45,858 - src.methods.zero_shot - INFO - Processing document 40/63: Unknown
2025-10-05 18:30:45,858 - src.methods.zero_shot - ERROR - Failed to process document 40: 'company'
2025-10-05 18:30:45,858 - src.methods.zero_shot - INFO - Processing document 41/63: Unknown
2025-10-05 18:30:45,858 - src.methods.zero_shot - ERROR - Failed to process document 41: 'company'
2025-10-05 18:30:45,858 - src.methods.zero_shot - INFO - Processing document 42/63: Unknown
2025-10-05 18:30:45,859 - src.methods.zero_shot - ERROR - Failed to process document 42: 'company'
2025-10-05 18:30:45,859 - src.methods.zero_shot - INFO - Processing document 43/63: Unknown
2025-10-05 18:30:45,859 - src.methods.zero_shot - ERROR - Failed to process document 43: 'company'
2025-10-05 18:30:45,859 - src.methods.zero_shot - INFO - Processing document 44/63: Unknown
2025-10-05 18:30:45,859 - src.methods.zero_shot - ERROR - Failed to process document 44: 'company'
2025-10-05 18:30:45,860 - src.methods.zero_shot - INFO - Processing document 45/63: Unknown
2025-10-05 18:30:45,860 - src.methods.zero_shot - ERROR - Failed to process document 45: 'company'
2025-10-05 18:30:45,860 - src.methods.zero_shot - INFO - Processing document 46/63: Unknown
2025-10-05 18:30:45,860 - src.methods.zero_shot - ERROR - Failed to process document 46: 'company'
2025-10-05 18:30:45,860 - src.methods.zero_shot - INFO - Processing document 47/63: Unknown
2025-10-05 18:30:45,860 - src.methods.zero_shot - ERROR - Failed to process document 47: 'company'
2025-10-05 18:30:45,861 - src.methods.zero_shot - INFO - Processing document 48/63: Unknown
2025-10-05 18:30:45,861 - src.methods.zero_shot - ERROR - Failed to process document 48: 'company'
2025-10-05 18:30:45,861 - src.methods.zero_shot - INFO - Processing document 49/63: Unknown
2025-10-05 18:30:45,861 - src.methods.zero_shot - ERROR - Failed to process document 49: 'company'
2025-10-05 18:30:45,861 - src.methods.zero_shot - INFO - Processing document 50/63: Unknown
2025-10-05 18:30:45,861 - src.methods.zero_shot - ERROR - Failed to process document 50: 'company'
2025-10-05 18:30:45,861 - src.methods.zero_shot - INFO - Processing document 51/63: Unknown
2025-10-05 18:30:45,862 - src.methods.zero_shot - ERROR - Failed to process document 51: 'company'
2025-10-05 18:30:45,862 - src.methods.zero_shot - INFO - Processing document 52/63: Unknown
2025-10-05 18:30:45,862 - src.methods.zero_shot - ERROR - Failed to process document 52: 'company'
2025-10-05 18:30:45,862 - src.methods.zero_shot - INFO - Processing document 53/63: Unknown
2025-10-05 18:30:45,863 - src.methods.zero_shot - ERROR - Failed to process document 53: 'company'
2025-10-05 18:30:45,863 - src.methods.zero_shot - INFO - Processing document 54/63: Unknown
2025-10-05 18:30:45,863 - src.methods.zero_shot - ERROR - Failed to process document 54: 'company'
2025-10-05 18:30:45,864 - src.methods.zero_shot - INFO - Processing document 55/63: Unknown
2025-10-05 18:30:45,864 - src.methods.zero_shot - ERROR - Failed to process document 55: 'company'
2025-10-05 18:30:45,864 - src.methods.zero_shot - INFO - Processing document 56/63: Unknown
2025-10-05 18:30:45,865 - src.methods.zero_shot - ERROR - Failed to process document 56: 'company'
2025-10-05 18:30:45,865 - src.methods.zero_shot - INFO - Processing document 57/63: Unknown
2025-10-05 18:30:45,865 - src.methods.zero_shot - ERROR - Failed to process document 57: 'company'
2025-10-05 18:30:45,865 - src.methods.zero_shot - INFO - Processing document 58/63: Unknown
2025-10-05 18:30:45,866 - src.methods.zero_shot - ERROR - Failed to process document 58: 'company'
2025-10-05 18:30:45,866 - src.methods.zero_shot - INFO - Processing document 59/63: Unknown
2025-10-05 18:30:45,866 - src.methods.zero_shot - ERROR - Failed to process document 59: 'company'
2025-10-05 18:30:45,866 - src.methods.zero_shot - INFO - Processing document 60/63: Unknown
2025-10-05 18:30:45,867 - src.methods.zero_shot - ERROR - Failed to process document 60: 'company'
2025-10-05 18:30:45,867 - src.methods.zero_shot - INFO - Processing document 61/63: Unknown
2025-10-05 18:30:45,867 - src.methods.zero_shot - ERROR - Failed to process document 61: 'company'
2025-10-05 18:30:45,867 - src.methods.zero_shot - INFO - Processing document 62/63: Unknown
2025-10-05 18:30:45,868 - src.methods.zero_shot - ERROR - Failed to process document 62: 'company'
2025-10-05 18:30:45,868 - src.methods.zero_shot - INFO - Processing document 63/63: Unknown
2025-10-05 18:30:45,868 - src.methods.zero_shot - ERROR - Failed to process document 63: 'company'
2025-10-05 18:30:45,870 - src.methods.zero_shot - INFO - Batch extraction complete. Total cost: $0.0000
2025-10-05 18:30:45,870 - experiment.financial_rag_pipeline - INFO - Metrics:
2025-10-05 18:30:45,871 - experiment.financial_rag_pipeline - INFO -   zero_shot_model: llama3-8b-8192
2025-10-05 18:30:45,871 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_cost: 0.0
2025-10-05 18:30:45,871 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_tokens: 0
2025-10-05 18:30:45,872 - experiment.financial_rag_pipeline - INFO -   zero_shot_num_requests: 0
2025-10-05 18:30:45,872 - src.utils.helpers - INFO - run_zero_shot_extraction took 0.05 seconds
2025-10-05 18:41:58,914 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:41:58,915 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:41:59,236 - __main__ - INFO - Groq client initialized
2025-10-05 18:41:59,296 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:41:59,296 - __main__ - INFO - ================================================================================
2025-10-05 18:41:59,297 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:41:59,297 - __main__ - INFO - ================================================================================
2025-10-05 18:41:59,298 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:41:59,299 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 18:41:59,299 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 18:41:59,621 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:41:59,623 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:41:59,623 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:01,660 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:01,660 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:01,660 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:05,709 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:05,710 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:05,711 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:05,711 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.41 seconds
2025-10-05 18:42:05,712 - src.methods.zero_shot - ERROR - Failed to process document 1: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:05,712 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 18:42:05,713 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 18:42:05,753 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:05,754 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:05,755 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:07,784 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:07,784 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:07,784 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:11,821 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:11,823 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:11,824 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:11,824 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.11 seconds
2025-10-05 18:42:11,825 - src.methods.zero_shot - ERROR - Failed to process document 2: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:11,825 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 18:42:11,826 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 18:42:11,862 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:11,863 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:11,863 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:13,902 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:13,904 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:13,904 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:17,949 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:17,950 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:17,951 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:17,951 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.12 seconds
2025-10-05 18:42:17,952 - src.methods.zero_shot - ERROR - Failed to process document 3: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:17,952 - src.methods.zero_shot - INFO - Processing document 4/63: BRITANNIA
2025-10-05 18:42:17,953 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 18:42:17,989 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:17,991 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:17,991 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:20,029 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:20,030 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:20,031 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:24,075 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:24,076 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:24,077 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:24,077 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.12 seconds
2025-10-05 18:42:24,078 - src.methods.zero_shot - ERROR - Failed to process document 4: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:24,078 - src.methods.zero_shot - INFO - Processing document 5/63: BRITANNIA
2025-10-05 18:42:24,079 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 18:42:24,109 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:24,110 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:24,111 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:26,140 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:26,140 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:26,140 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:30,241 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:30,242 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:30,243 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:30,243 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.16 seconds
2025-10-05 18:42:30,244 - src.methods.zero_shot - ERROR - Failed to process document 5: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:30,244 - src.methods.zero_shot - INFO - Processing document 6/63: BRITANNIA
2025-10-05 18:42:30,245 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2025 (Zero-shot)
2025-10-05 18:42:30,271 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:30,271 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:30,271 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:32,311 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:32,313 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:32,315 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:36,369 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:36,370 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:36,371 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:36,372 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.13 seconds
2025-10-05 18:42:36,372 - src.methods.zero_shot - ERROR - Failed to process document 6: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:36,373 - src.methods.zero_shot - INFO - Processing document 7/63: BRITANNIA
2025-10-05 18:42:36,374 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2024 (Zero-shot)
2025-10-05 18:42:36,424 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:36,425 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:36,426 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:38,471 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:38,473 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:38,473 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:42,522 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:42,522 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:42,522 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:42,522 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.15 seconds
2025-10-05 18:42:42,522 - src.methods.zero_shot - ERROR - Failed to process document 7: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:42,522 - src.methods.zero_shot - INFO - Processing document 8/63: BRITANNIA
2025-10-05 18:42:42,522 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2025 (Zero-shot)
2025-10-05 18:42:42,571 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:42,571 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:42,571 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:44,621 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:44,623 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:44,623 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:48,668 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:48,669 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:48,670 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:42:48,670 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.15 seconds
2025-10-05 18:42:48,671 - src.methods.zero_shot - ERROR - Failed to process document 8: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:48,672 - src.methods.zero_shot - INFO - Processing document 9/63: DABUR
2025-10-05 18:42:48,672 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2024 (Zero-shot)
2025-10-05 18:42:48,708 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:48,708 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:48,709 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:42:50,736 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:42:50,736 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:42:50,736 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:42:51,768 - src.utils.helpers - INFO - LLM call for DABUR took 3.10 seconds
2025-10-05 18:42:51,769 - src.utils.helpers - INFO - run_zero_shot_extraction took 52.47 seconds
2025-10-05 18:42:51,769 - __main__ - WARNING - 
Pipeline interrupted by user
2025-10-05 18:45:03,461 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:45:03,461 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:45:03,755 - __main__ - INFO - Groq client initialized
2025-10-05 18:45:03,813 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:45:03,814 - __main__ - INFO - ================================================================================
2025-10-05 18:45:03,814 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:45:03,815 - __main__ - INFO - ================================================================================
2025-10-05 18:45:03,815 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:45:03,816 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 18:45:03,816 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 18:45:03,983 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:03,983 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:03,984 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:06,036 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:06,037 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:06,037 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:10,068 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:10,069 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:10,070 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:10,070 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.25 seconds
2025-10-05 18:45:10,070 - src.methods.zero_shot - ERROR - Failed to process document 1: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9652, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:10,070 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 18:45:10,070 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 18:45:10,110 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:10,111 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:10,111 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:12,156 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:12,158 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:12,158 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:16,180 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:16,195 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:16,195 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:16,195 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.13 seconds
2025-10-05 18:45:16,195 - src.methods.zero_shot - ERROR - Failed to process document 2: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7384, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:16,195 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 18:45:16,195 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 18:45:16,227 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:16,227 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:16,227 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:18,290 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:18,292 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:18,292 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:22,332 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:22,336 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:22,337 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:22,337 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.14 seconds
2025-10-05 18:45:22,338 - src.methods.zero_shot - ERROR - Failed to process document 3: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 12559, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:22,338 - src.methods.zero_shot - INFO - Processing document 4/63: BRITANNIA
2025-10-05 18:45:22,339 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 18:45:22,370 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:22,371 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:22,371 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:24,410 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:24,412 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:24,412 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:28,461 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:28,462 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:28,462 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:28,462 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.12 seconds
2025-10-05 18:45:28,463 - src.methods.zero_shot - ERROR - Failed to process document 4: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10923, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:28,463 - src.methods.zero_shot - INFO - Processing document 5/63: BRITANNIA
2025-10-05 18:45:28,464 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 18:45:28,501 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:28,503 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:28,504 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:30,564 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:30,564 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:30,564 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:34,605 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:34,607 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:34,607 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:34,607 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.14 seconds
2025-10-05 18:45:34,608 - src.methods.zero_shot - ERROR - Failed to process document 5: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 10658, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:34,609 - src.methods.zero_shot - INFO - Processing document 6/63: BRITANNIA
2025-10-05 18:45:34,609 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2025 (Zero-shot)
2025-10-05 18:45:34,638 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:34,639 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:34,640 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:36,681 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:36,681 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:36,681 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:41,098 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:41,100 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:41,100 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:41,100 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.49 seconds
2025-10-05 18:45:41,100 - src.methods.zero_shot - ERROR - Failed to process document 6: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8044, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:41,100 - src.methods.zero_shot - INFO - Processing document 7/63: BRITANNIA
2025-10-05 18:45:41,101 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2024 (Zero-shot)
2025-10-05 18:45:41,177 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:41,178 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:41,178 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:43,229 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:43,229 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:43,229 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:47,283 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:47,284 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:47,285 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:47,285 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.18 seconds
2025-10-05 18:45:47,286 - src.methods.zero_shot - ERROR - Failed to process document 7: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 17454, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:47,286 - src.methods.zero_shot - INFO - Processing document 8/63: BRITANNIA
2025-10-05 18:45:47,287 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2025 (Zero-shot)
2025-10-05 18:45:47,330 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:47,332 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:47,332 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:49,379 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:49,379 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:49,379 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:53,419 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:53,420 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:53,421 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:53,421 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.13 seconds
2025-10-05 18:45:53,422 - src.methods.zero_shot - ERROR - Failed to process document 8: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 14233, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:53,422 - src.methods.zero_shot - INFO - Processing document 9/63: DABUR
2025-10-05 18:45:53,422 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2024 (Zero-shot)
2025-10-05 18:45:53,452 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:53,453 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:53,454 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:45:55,488 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:55,489 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:55,490 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:45:59,524 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:59,524 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:59,524 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:45:59,524 - src.utils.helpers - INFO - LLM call for DABUR took 6.10 seconds
2025-10-05 18:45:59,524 - src.methods.zero_shot - ERROR - Failed to process document 9: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7979, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:59,524 - src.methods.zero_shot - INFO - Processing document 10/63: DABUR
2025-10-05 18:45:59,524 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2025 (Zero-shot)
2025-10-05 18:45:59,565 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:45:59,566 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:45:59,567 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:46:01,601 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:46:01,601 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:46:01,601 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:46:05,649 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:46:05,649 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:46:05,649 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:46:05,649 - src.utils.helpers - INFO - LLM call for DABUR took 6.12 seconds
2025-10-05 18:46:05,649 - src.methods.zero_shot - ERROR - Failed to process document 10: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 8093, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:46:05,649 - src.methods.zero_shot - INFO - Processing document 11/63: DABUR
2025-10-05 18:46:05,649 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q2 2024 (Zero-shot)
2025-10-05 18:46:05,703 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:46:05,704 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 11109, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:46:05,705 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 11109, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 18:46:07,743 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-10-05 18:46:07,743 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 11109, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 18:46:07,743 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 11109, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 18:46:09,207 - src.utils.helpers - INFO - LLM call for DABUR took 3.56 seconds
2025-10-05 18:46:09,207 - src.utils.helpers - INFO - run_zero_shot_extraction took 65.39 seconds
2025-10-05 18:46:09,207 - __main__ - WARNING - 
Pipeline interrupted by user
2025-10-05 18:46:38,085 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:46:38,085 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:46:38,391 - __main__ - INFO - Groq client initialized
2025-10-05 18:46:38,447 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:46:38,448 - __main__ - INFO - ================================================================================
2025-10-05 18:46:38,448 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:46:38,449 - __main__ - INFO - ================================================================================
2025-10-05 18:46:38,449 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:46:38,450 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 18:46:38,450 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 18:46:38,758 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:38,758 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:38,758 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:46:40,808 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:40,809 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:40,810 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:46:44,850 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:44,851 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:44,852 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:46:44,852 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.40 seconds
2025-10-05 18:46:44,853 - src.methods.zero_shot - ERROR - Failed to process document 1: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:44,853 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 18:46:44,854 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 18:46:44,888 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:44,890 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:44,891 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:46:46,927 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:46,929 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:46,930 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:46:50,967 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:50,969 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:50,969 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:46:50,970 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.11 seconds
2025-10-05 18:46:50,970 - src.methods.zero_shot - ERROR - Failed to process document 2: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:50,970 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 18:46:50,971 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 18:46:51,028 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:46:51,029 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:46:51,030 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:46:52,704 - src.utils.helpers - INFO - LLM call for BRITANNIA took 1.73 seconds
2025-10-05 18:46:52,706 - src.utils.helpers - INFO - run_zero_shot_extraction took 14.26 seconds
2025-10-05 18:46:52,706 - __main__ - WARNING - 
Pipeline interrupted by user
2025-10-05 18:52:39,048 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:52:39,049 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:52:39,428 - __main__ - INFO - Groq client initialized
2025-10-05 18:52:39,500 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:52:39,501 - __main__ - INFO - ================================================================================
2025-10-05 18:52:39,501 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:52:39,502 - __main__ - INFO - ================================================================================
2025-10-05 18:52:39,502 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:52:39,503 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 18:52:39,503 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 18:52:39,504 - src.methods.zero_shot - WARNING - Truncating text from 36706 to 20000 chars
2025-10-05 18:52:40,092 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:40,094 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:40,094 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:52:42,152 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:42,153 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:42,153 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:52:46,222 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:46,223 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:46,224 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:52:46,224 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.72 seconds
2025-10-05 18:52:46,225 - src.methods.zero_shot - ERROR - Failed to process document 1: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:46,225 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 18:52:46,226 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 18:52:46,226 - src.methods.zero_shot - WARNING - Truncating text from 27637 to 20000 chars
2025-10-05 18:52:46,261 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:46,262 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:46,263 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:52:48,368 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:48,368 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:48,384 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:52:52,422 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:52,424 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:52,425 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:52:52,425 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.20 seconds
2025-10-05 18:52:52,426 - src.methods.zero_shot - ERROR - Failed to process document 2: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:52,427 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 18:52:52,427 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 18:52:52,428 - src.methods.zero_shot - WARNING - Truncating text from 48338 to 20000 chars
2025-10-05 18:52:52,457 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:52,458 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:52,459 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:52:54,495 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:54,496 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:54,497 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:52:58,523 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:58,523 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:58,523 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:52:58,523 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.10 seconds
2025-10-05 18:52:58,523 - src.methods.zero_shot - ERROR - Failed to process document 3: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:58,523 - src.methods.zero_shot - INFO - Processing document 4/63: BRITANNIA
2025-10-05 18:52:58,523 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 18:52:58,523 - src.methods.zero_shot - WARNING - Truncating text from 41769 to 20000 chars
2025-10-05 18:52:58,560 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:52:58,562 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:52:58,562 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:53:00,601 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:00,602 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:00,603 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:53:04,636 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:04,637 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:04,638 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:53:04,638 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.11 seconds
2025-10-05 18:53:04,639 - src.methods.zero_shot - ERROR - Failed to process document 4: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:04,639 - src.methods.zero_shot - INFO - Processing document 5/63: BRITANNIA
2025-10-05 18:53:04,640 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 18:53:04,640 - src.methods.zero_shot - WARNING - Truncating text from 40740 to 20000 chars
2025-10-05 18:53:04,664 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:04,665 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:04,666 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:53:06,714 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:06,715 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:06,716 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:53:10,753 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:10,755 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:10,756 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:53:10,756 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.12 seconds
2025-10-05 18:53:10,756 - src.methods.zero_shot - ERROR - Failed to process document 5: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:10,756 - src.methods.zero_shot - INFO - Processing document 6/63: BRITANNIA
2025-10-05 18:53:10,756 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2025 (Zero-shot)
2025-10-05 18:53:10,756 - src.methods.zero_shot - WARNING - Truncating text from 30274 to 20000 chars
2025-10-05 18:53:10,774 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:10,786 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:10,786 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:53:12,819 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:12,819 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:12,819 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:53:16,852 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:16,852 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:16,852 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:53:16,852 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.10 seconds
2025-10-05 18:53:16,867 - src.methods.zero_shot - ERROR - Failed to process document 6: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:16,867 - src.methods.zero_shot - INFO - Processing document 7/63: BRITANNIA
2025-10-05 18:53:16,867 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2024 (Zero-shot)
2025-10-05 18:53:16,867 - src.methods.zero_shot - WARNING - Truncating text from 67837 to 20000 chars
2025-10-05 18:53:16,897 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:16,898 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:16,898 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:53:19,151 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:19,152 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:19,153 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:53:23,188 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:23,190 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:23,190 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:53:23,191 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.32 seconds
2025-10-05 18:53:23,191 - src.methods.zero_shot - ERROR - Failed to process document 7: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:23,192 - src.methods.zero_shot - INFO - Processing document 8/63: BRITANNIA
2025-10-05 18:53:23,192 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2025 (Zero-shot)
2025-10-05 18:53:23,193 - src.methods.zero_shot - WARNING - Truncating text from 54957 to 20000 chars
2025-10-05 18:53:23,220 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:53:23,222 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:53:23,223 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:53:23,495 - src.utils.helpers - INFO - LLM call for BRITANNIA took 0.30 seconds
2025-10-05 18:53:23,496 - src.utils.helpers - INFO - run_zero_shot_extraction took 43.99 seconds
2025-10-05 18:53:23,496 - __main__ - WARNING - 
Pipeline interrupted by user
2025-10-05 18:55:31,999 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 18:55:32,000 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 18:55:32,318 - __main__ - INFO - Groq client initialized
2025-10-05 18:55:32,383 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 18:55:32,384 - __main__ - INFO - ================================================================================
2025-10-05 18:55:32,385 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 18:55:32,385 - __main__ - INFO - ================================================================================
2025-10-05 18:55:32,386 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 18:55:32,386 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 18:55:32,387 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 18:55:32,387 - src.methods.zero_shot - WARNING - Truncating text from 36706 to 20000 chars
2025-10-05 18:55:32,601 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:32,601 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:32,601 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:55:34,617 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:34,617 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:34,617 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:55:38,633 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:38,649 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:38,649 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:55:38,650 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.26 seconds
2025-10-05 18:55:38,650 - src.methods.zero_shot - ERROR - Failed to process document 1: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:38,651 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 18:55:38,651 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 18:55:38,652 - src.methods.zero_shot - WARNING - Truncating text from 27637 to 20000 chars
2025-10-05 18:55:38,679 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:38,680 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:38,680 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:55:40,711 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:40,711 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:40,711 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:55:44,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:44,727 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:44,727 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:55:44,727 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.07 seconds
2025-10-05 18:55:44,727 - src.methods.zero_shot - ERROR - Failed to process document 2: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:44,742 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 18:55:44,742 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 18:55:44,742 - src.methods.zero_shot - WARNING - Truncating text from 48338 to 20000 chars
2025-10-05 18:55:44,768 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:44,770 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:44,771 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:55:46,796 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:46,797 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:46,798 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:55:50,836 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:50,836 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:50,836 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:55:50,836 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.09 seconds
2025-10-05 18:55:50,836 - src.methods.zero_shot - ERROR - Failed to process document 3: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:50,836 - src.methods.zero_shot - INFO - Processing document 4/63: BRITANNIA
2025-10-05 18:55:50,836 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 18:55:50,836 - src.methods.zero_shot - WARNING - Truncating text from 41769 to 20000 chars
2025-10-05 18:55:50,874 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:50,875 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:50,876 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:55:52,911 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:52,912 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:52,913 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:55:56,934 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:56,934 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:56,934 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 18:55:56,934 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.10 seconds
2025-10-05 18:55:56,934 - src.methods.zero_shot - ERROR - Failed to process document 4: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:56,934 - src.methods.zero_shot - INFO - Processing document 5/63: BRITANNIA
2025-10-05 18:55:56,934 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 18:55:56,934 - src.methods.zero_shot - WARNING - Truncating text from 40740 to 20000 chars
2025-10-05 18:55:56,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:56,978 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:56,979 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 2.0s...
2025-10-05 18:55:59,015 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-05 18:55:59,017 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-10-05 18:55:59,017 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}. Retrying in 4.0s...
2025-10-05 18:55:59,234 - src.utils.helpers - INFO - LLM call for BRITANNIA took 2.30 seconds
2025-10-05 18:55:59,234 - src.utils.helpers - INFO - run_zero_shot_extraction took 26.85 seconds
2025-10-05 18:55:59,235 - __main__ - WARNING - 
Pipeline interrupted by user
2025-10-05 19:01:12,977 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-05 19:01:12,977 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-05 19:01:13,446 - __main__ - INFO - Groq client initialized
2025-10-05 19:01:13,548 - __main__ - INFO - Loaded 63 documents for extraction
2025-10-05 19:01:13,548 - __main__ - INFO - ================================================================================
2025-10-05 19:01:13,548 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2025-10-05 19:01:13,548 - __main__ - INFO - ================================================================================
2025-10-05 19:01:13,548 - src.methods.zero_shot - INFO - Starting batch extraction for 63 documents
2025-10-05 19:01:13,548 - src.methods.zero_shot - INFO - Processing document 1/63: BRITANNIA
2025-10-05 19:01:13,548 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 19:01:13,548 - src.methods.zero_shot - WARNING - Truncating text from 36706 to 20000 chars
2025-10-05 19:01:17,781 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:01:17,830 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:01:17,830 - src.methods.zero_shot - INFO - API call completed in 4.28s, cost: $0.0104
2025-10-05 19:01:17,830 - src.utils.helpers - INFO - LLM call for BRITANNIA took 4.28 seconds
2025-10-05 19:01:17,830 - src.methods.zero_shot - INFO - Processing document 2/63: BRITANNIA
2025-10-05 19:01:17,830 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 19:01:17,830 - src.methods.zero_shot - WARNING - Truncating text from 27637 to 20000 chars
2025-10-05 19:01:17,854 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:01:17,854 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 2.000000 seconds
2025-10-05 19:01:21,171 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:01:21,171 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:01:21,171 - src.methods.zero_shot - INFO - API call completed in 3.34s, cost: $0.0100
2025-10-05 19:01:21,187 - src.utils.helpers - INFO - LLM call for BRITANNIA took 3.36 seconds
2025-10-05 19:01:21,191 - src.methods.zero_shot - INFO - Processing document 3/63: BRITANNIA
2025-10-05 19:01:21,191 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 19:01:21,191 - src.methods.zero_shot - WARNING - Truncating text from 48338 to 20000 chars
2025-10-05 19:01:21,221 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:01:21,221 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2025-10-05 19:01:54,805 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:01:54,813 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:01:54,813 - src.methods.zero_shot - INFO - API call completed in 33.62s, cost: $0.0108
2025-10-05 19:01:54,813 - src.utils.helpers - INFO - LLM call for BRITANNIA took 33.62 seconds
2025-10-05 19:01:54,813 - src.methods.zero_shot - INFO - Processing document 4/63: BRITANNIA
2025-10-05 19:01:54,813 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 19:01:54,813 - src.methods.zero_shot - WARNING - Truncating text from 41769 to 20000 chars
2025-10-05 19:01:54,845 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:01:54,845 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2025-10-05 19:02:30,148 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:02:30,148 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:02:30,148 - src.methods.zero_shot - INFO - API call completed in 35.34s, cost: $0.0102
2025-10-05 19:02:30,148 - src.utils.helpers - INFO - LLM call for BRITANNIA took 35.34 seconds
2025-10-05 19:02:30,148 - src.methods.zero_shot - INFO - Processing document 5/63: BRITANNIA
2025-10-05 19:02:30,148 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 19:02:30,148 - src.methods.zero_shot - WARNING - Truncating text from 40740 to 20000 chars
2025-10-05 19:02:30,180 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:02:30,180 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2025-10-05 19:03:03,518 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:03:03,526 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:03:03,526 - src.methods.zero_shot - INFO - API call completed in 33.38s, cost: $0.0104
2025-10-05 19:03:03,526 - src.utils.helpers - INFO - LLM call for BRITANNIA took 33.38 seconds
2025-10-05 19:03:03,526 - src.methods.zero_shot - INFO - Processing document 6/63: BRITANNIA
2025-10-05 19:03:03,526 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2025 (Zero-shot)
2025-10-05 19:03:03,526 - src.methods.zero_shot - WARNING - Truncating text from 30274 to 20000 chars
2025-10-05 19:03:03,701 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:03:03,709 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 33.000000 seconds
2025-10-05 19:03:37,992 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:03:37,992 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:03:37,992 - src.methods.zero_shot - INFO - API call completed in 34.47s, cost: $0.0107
2025-10-05 19:03:37,992 - src.utils.helpers - INFO - LLM call for BRITANNIA took 34.47 seconds
2025-10-05 19:03:38,005 - src.methods.zero_shot - INFO - Processing document 7/63: BRITANNIA
2025-10-05 19:03:38,005 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2024 (Zero-shot)
2025-10-05 19:03:38,005 - src.methods.zero_shot - WARNING - Truncating text from 67837 to 20000 chars
2025-10-05 19:03:38,031 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:03:38,031 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 33.000000 seconds
2025-10-05 19:04:12,323 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:04:12,323 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:04:12,323 - src.methods.zero_shot - INFO - API call completed in 34.32s, cost: $0.0108
2025-10-05 19:04:12,331 - src.utils.helpers - INFO - LLM call for BRITANNIA took 34.33 seconds
2025-10-05 19:04:12,333 - src.methods.zero_shot - INFO - Processing document 8/63: BRITANNIA
2025-10-05 19:04:12,333 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2025 (Zero-shot)
2025-10-05 19:04:12,333 - src.methods.zero_shot - WARNING - Truncating text from 54957 to 20000 chars
2025-10-05 19:04:12,360 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:04:12,360 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2025-10-05 19:04:47,578 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:04:47,586 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:04:47,586 - src.methods.zero_shot - INFO - API call completed in 35.25s, cost: $0.0100
2025-10-05 19:04:47,586 - src.utils.helpers - INFO - LLM call for BRITANNIA took 35.25 seconds
2025-10-05 19:04:47,594 - src.methods.zero_shot - INFO - Processing document 9/63: DABUR
2025-10-05 19:04:47,594 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2024 (Zero-shot)
2025-10-05 19:04:47,594 - src.methods.zero_shot - WARNING - Truncating text from 30096 to 20000 chars
2025-10-05 19:04:47,632 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:04:47,633 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2025-10-05 19:05:20,838 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:05:20,839 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:05:20,840 - src.methods.zero_shot - INFO - API call completed in 33.25s, cost: $0.0099
2025-10-05 19:05:20,840 - src.utils.helpers - INFO - LLM call for DABUR took 33.25 seconds
2025-10-05 19:05:20,844 - src.methods.zero_shot - INFO - Processing document 10/63: DABUR
2025-10-05 19:05:20,845 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2025 (Zero-shot)
2025-10-05 19:05:20,845 - src.methods.zero_shot - WARNING - Truncating text from 30565 to 20000 chars
2025-10-05 19:05:20,875 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:05:20,875 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 31.000000 seconds
2025-10-05 19:05:53,115 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:05:53,115 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:05:53,115 - src.methods.zero_shot - INFO - API call completed in 32.27s, cost: $0.0099
2025-10-05 19:05:53,115 - src.utils.helpers - INFO - LLM call for DABUR took 32.27 seconds
2025-10-05 19:05:53,119 - src.methods.zero_shot - INFO - Processing document 11/63: DABUR
2025-10-05 19:05:53,119 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q2 2024 (Zero-shot)
2025-10-05 19:05:53,119 - src.methods.zero_shot - WARNING - Truncating text from 42616 to 20000 chars
2025-10-05 19:05:53,196 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:05:53,197 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 31.000000 seconds
2025-10-05 19:06:25,496 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:06:25,498 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:06:25,499 - src.methods.zero_shot - INFO - API call completed in 32.38s, cost: $0.0110
2025-10-05 19:06:25,500 - src.utils.helpers - INFO - LLM call for DABUR took 32.38 seconds
2025-10-05 19:06:25,502 - src.methods.zero_shot - INFO - Processing document 12/63: DABUR
2025-10-05 19:06:25,502 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q2 2025 (Zero-shot)
2025-10-05 19:06:25,502 - src.methods.zero_shot - WARNING - Truncating text from 43845 to 20000 chars
2025-10-05 19:06:25,531 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:06:25,532 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 35.000000 seconds
2025-10-05 19:07:01,780 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:07:01,780 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:07:01,794 - src.methods.zero_shot - INFO - API call completed in 36.29s, cost: $0.0113
2025-10-05 19:07:01,794 - src.utils.helpers - INFO - LLM call for DABUR took 36.29 seconds
2025-10-05 19:07:01,796 - src.methods.zero_shot - INFO - Processing document 13/63: DABUR
2025-10-05 19:07:01,828 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q3 2024 (Zero-shot)
2025-10-05 19:07:01,829 - src.methods.zero_shot - WARNING - Truncating text from 33178 to 20000 chars
2025-10-05 19:07:01,869 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:07:01,870 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 36.000000 seconds
2025-10-05 19:07:39,117 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:07:39,117 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:07:39,117 - src.methods.zero_shot - INFO - API call completed in 37.29s, cost: $0.0106
2025-10-05 19:07:39,117 - src.utils.helpers - INFO - LLM call for DABUR took 37.29 seconds
2025-10-05 19:07:39,132 - src.methods.zero_shot - INFO - Processing document 14/63: DABUR
2025-10-05 19:07:39,132 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q3 2025 (Zero-shot)
2025-10-05 19:07:39,132 - src.methods.zero_shot - WARNING - Truncating text from 34607 to 20000 chars
2025-10-05 19:07:39,225 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:07:39,226 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2025-10-05 19:08:14,415 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:08:14,415 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:08:14,415 - src.methods.zero_shot - INFO - API call completed in 35.28s, cost: $0.0106
2025-10-05 19:08:14,415 - src.utils.helpers - INFO - LLM call for DABUR took 35.28 seconds
2025-10-05 19:08:14,430 - src.methods.zero_shot - INFO - Processing document 15/63: DABUR
2025-10-05 19:08:14,430 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q4 2024 (Zero-shot)
2025-10-05 19:08:14,430 - src.methods.zero_shot - WARNING - Truncating text from 28653 to 20000 chars
2025-10-05 19:08:14,470 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:14,471 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95940, Requested 5483. Please try again in 20m29.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:14,472 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95940, Requested 5483. Please try again in 20m29.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:16,508 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:16,508 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95938, Requested 5483. Please try again in 20m27.386s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:16,508 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95938, Requested 5483. Please try again in 20m27.386s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:20,570 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:20,571 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5483. Please try again in 20m23.322999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:20,572 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:20,572 - src.utils.helpers - INFO - LLM call for DABUR took 6.14 seconds
2025-10-05 19:08:20,573 - src.methods.zero_shot - ERROR - Failed to process document 15: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5483. Please try again in 20m23.322999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:20,573 - src.methods.zero_shot - INFO - Processing document 16/63: DABUR
2025-10-05 19:08:20,574 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q4 2025 (Zero-shot)
2025-10-05 19:08:20,574 - src.methods.zero_shot - WARNING - Truncating text from 54253 to 20000 chars
2025-10-05 19:08:20,621 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:20,622 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5462. Please try again in 20m5.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:20,623 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5462. Please try again in 20m5.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:22,674 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:22,675 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95931, Requested 5462. Please try again in 20m3.073999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:22,676 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95931, Requested 5462. Please try again in 20m3.073999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:26,710 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:26,710 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 5462. Please try again in 19m59.037s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:26,710 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:26,710 - src.utils.helpers - INFO - LLM call for DABUR took 6.14 seconds
2025-10-05 19:08:26,710 - src.methods.zero_shot - ERROR - Failed to process document 16: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 5462. Please try again in 19m59.037s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:26,710 - src.methods.zero_shot - INFO - Processing document 17/63: MARICO
2025-10-05 19:08:26,710 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q1 2024 (Zero-shot)
2025-10-05 19:08:26,749 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:26,751 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 4990. Please try again in 13m11.189999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:26,752 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 4990. Please try again in 13m11.189999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:28,784 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:28,784 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95924, Requested 4990. Please try again in 13m9.156999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:28,784 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95924, Requested 4990. Please try again in 13m9.156999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:32,817 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:32,818 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4990. Please try again in 13m5.123s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:32,819 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:32,819 - src.utils.helpers - INFO - LLM call for MARICO took 6.11 seconds
2025-10-05 19:08:32,820 - src.methods.zero_shot - ERROR - Failed to process document 17: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4990. Please try again in 13m5.123s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:32,821 - src.methods.zero_shot - INFO - Processing document 18/63: MARICO
2025-10-05 19:08:32,821 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q1 2025 (Zero-shot)
2025-10-05 19:08:32,851 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:32,852 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4456. Please try again in 5m23.712999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:32,853 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4456. Please try again in 5m23.712999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:34,885 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:34,885 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95917, Requested 4456. Please try again in 5m21.668s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:34,899 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95917, Requested 4456. Please try again in 5m21.668s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:38,919 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:38,919 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4456. Please try again in 5m17.639999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:38,919 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:38,919 - src.utils.helpers - INFO - LLM call for MARICO took 6.10 seconds
2025-10-05 19:08:38,919 - src.methods.zero_shot - ERROR - Failed to process document 18: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4456. Please try again in 5m17.639999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:38,919 - src.methods.zero_shot - INFO - Processing document 19/63: MARICO
2025-10-05 19:08:38,919 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q2 2024 (Zero-shot)
2025-10-05 19:08:38,919 - src.methods.zero_shot - WARNING - Truncating text from 34724 to 20000 chars
2025-10-05 19:08:38,956 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:38,957 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 5479. Please try again in 20m1.480999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:38,958 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 5479. Please try again in 20m1.480999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:40,998 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:40,998 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95910, Requested 5479. Please try again in 19m59.438s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:40,998 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95910, Requested 5479. Please try again in 19m59.438s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:45,039 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:45,039 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95905, Requested 5479. Please try again in 19m55.400999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:45,039 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:45,039 - src.utils.helpers - INFO - LLM call for MARICO took 6.12 seconds
2025-10-05 19:08:45,039 - src.methods.zero_shot - ERROR - Failed to process document 19: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95905, Requested 5479. Please try again in 19m55.400999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:45,039 - src.methods.zero_shot - INFO - Processing document 20/63: MARICO
2025-10-05 19:08:45,039 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q2 2025 (Zero-shot)
2025-10-05 19:08:45,039 - src.methods.zero_shot - WARNING - Truncating text from 33292 to 20000 chars
2025-10-05 19:08:45,073 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:45,074 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95905, Requested 5476. Please try again in 19m52.771s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:45,075 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95905, Requested 5476. Please try again in 19m52.771s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:47,114 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:47,115 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95903, Requested 5476. Please try again in 19m50.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:47,116 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95903, Requested 5476. Please try again in 19m50.731s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:51,172 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:51,173 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95898, Requested 5476. Please try again in 19m46.671999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:51,174 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:51,174 - src.utils.helpers - INFO - LLM call for MARICO took 6.14 seconds
2025-10-05 19:08:51,175 - src.methods.zero_shot - ERROR - Failed to process document 20: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95898, Requested 5476. Please try again in 19m46.671999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:51,176 - src.methods.zero_shot - INFO - Processing document 21/63: MARICO
2025-10-05 19:08:51,176 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q3 2024 (Zero-shot)
2025-10-05 19:08:51,176 - src.methods.zero_shot - WARNING - Truncating text from 23625 to 20000 chars
2025-10-05 19:08:51,211 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:51,213 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95898, Requested 5490. Please try again in 19m58.730999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:51,214 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95898, Requested 5490. Please try again in 19m58.730999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:53,256 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:53,257 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95896, Requested 5490. Please try again in 19m56.686s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:53,258 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95896, Requested 5490. Please try again in 19m56.686s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:08:57,305 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:57,306 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95891, Requested 5490. Please try again in 19m52.636s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:57,307 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:08:57,307 - src.utils.helpers - INFO - LLM call for MARICO took 6.13 seconds
2025-10-05 19:08:57,307 - src.methods.zero_shot - ERROR - Failed to process document 21: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95891, Requested 5490. Please try again in 19m52.636s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:57,308 - src.methods.zero_shot - INFO - Processing document 22/63: MARICO
2025-10-05 19:08:57,308 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q3 2025 (Zero-shot)
2025-10-05 19:08:57,309 - src.methods.zero_shot - WARNING - Truncating text from 23900 to 20000 chars
2025-10-05 19:08:57,335 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:57,336 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95891, Requested 5485. Please try again in 19m48.285s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:57,338 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95891, Requested 5485. Please try again in 19m48.285s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:08:59,374 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:08:59,376 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95888, Requested 5485. Please try again in 19m46.245s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:08:59,376 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95888, Requested 5485. Please try again in 19m46.245s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:03,407 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:03,408 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95884, Requested 5485. Please try again in 19m42.214s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:03,409 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:03,409 - src.utils.helpers - INFO - LLM call for MARICO took 6.10 seconds
2025-10-05 19:09:03,410 - src.methods.zero_shot - ERROR - Failed to process document 22: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95884, Requested 5485. Please try again in 19m42.214s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:03,410 - src.methods.zero_shot - INFO - Processing document 23/63: MARICO
2025-10-05 19:09:03,410 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q4 2024 (Zero-shot)
2025-10-05 19:09:03,411 - src.methods.zero_shot - WARNING - Truncating text from 48274 to 20000 chars
2025-10-05 19:09:03,436 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:03,437 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95884, Requested 5488. Please try again in 19m44.775s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:03,437 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95884, Requested 5488. Please try again in 19m44.775s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:05,482 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:05,483 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95881, Requested 5488. Please try again in 19m42.729999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:05,484 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95881, Requested 5488. Please try again in 19m42.729999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:09,508 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:09,508 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95877, Requested 5488. Please try again in 19m38.690999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:09,508 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:09,524 - src.utils.helpers - INFO - LLM call for MARICO took 6.11 seconds
2025-10-05 19:09:09,524 - src.methods.zero_shot - ERROR - Failed to process document 23: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95877, Requested 5488. Please try again in 19m38.690999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:09,524 - src.methods.zero_shot - INFO - Processing document 24/63: MARICO
2025-10-05 19:09:09,524 - src.methods.zero_shot - INFO - Extracting metrics for MARICO Q4 2025 (Zero-shot)
2025-10-05 19:09:09,524 - src.methods.zero_shot - WARNING - Truncating text from 56216 to 20000 chars
2025-10-05 19:09:09,563 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:09,563 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95877, Requested 5494. Please try again in 19m43.832999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:09,563 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95877, Requested 5494. Please try again in 19m43.832999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:11,605 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:11,606 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95874, Requested 5494. Please try again in 19m41.791s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:11,607 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95874, Requested 5494. Please try again in 19m41.791s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:15,629 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:15,629 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95870, Requested 5494. Please try again in 19m37.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:15,629 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:15,629 - src.utils.helpers - INFO - LLM call for MARICO took 6.11 seconds
2025-10-05 19:09:15,629 - src.methods.zero_shot - ERROR - Failed to process document 24: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95870, Requested 5494. Please try again in 19m37.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:15,643 - src.methods.zero_shot - INFO - Processing document 25/63: TATACONSUM
2025-10-05 19:09:15,643 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q1 2024 (Zero-shot)
2025-10-05 19:09:16,166 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:09:16,168 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:09:16,169 - src.methods.zero_shot - INFO - API call completed in 0.52s, cost: $0.0022
2025-10-05 19:09:16,170 - src.utils.helpers - INFO - LLM call for TATACONSUM took 0.53 seconds
2025-10-05 19:09:16,172 - src.methods.zero_shot - INFO - Processing document 26/63: TATACONSUM
2025-10-05 19:09:16,173 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q1 2025 (Zero-shot)
2025-10-05 19:09:16,200 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:16,202 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97268, Requested 3070. Please try again in 4m51.998s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:16,203 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97268, Requested 3070. Please try again in 4m51.998s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:18,245 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:18,246 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97266, Requested 3070. Please try again in 4m49.953s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:18,246 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97266, Requested 3070. Please try again in 4m49.953s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:22,299 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:22,300 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97261, Requested 3070. Please try again in 4m45.897s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:22,300 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:22,301 - src.utils.helpers - INFO - LLM call for TATACONSUM took 6.13 seconds
2025-10-05 19:09:22,301 - src.methods.zero_shot - ERROR - Failed to process document 26: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97261, Requested 3070. Please try again in 4m45.897s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:22,301 - src.methods.zero_shot - INFO - Processing document 27/63: TATACONSUM
2025-10-05 19:09:22,302 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q2 2024 (Zero-shot)
2025-10-05 19:09:22,820 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:09:22,820 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:09:22,820 - src.methods.zero_shot - INFO - API call completed in 0.52s, cost: $0.0019
2025-10-05 19:09:22,820 - src.utils.helpers - INFO - LLM call for TATACONSUM took 0.52 seconds
2025-10-05 19:09:22,820 - src.methods.zero_shot - INFO - Processing document 28/63: TATACONSUM
2025-10-05 19:09:22,820 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q2 2025 (Zero-shot)
2025-10-05 19:09:22,820 - src.methods.zero_shot - WARNING - Truncating text from 42225 to 20000 chars
2025-10-05 19:09:22,850 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:22,851 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98461, Requested 5468. Please try again in 56m34.627s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:22,852 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98461, Requested 5468. Please try again in 56m34.627s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:24,876 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:24,893 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98459, Requested 5468. Please try again in 56m32.586999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:24,893 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98459, Requested 5468. Please try again in 56m32.586999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:28,926 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:28,927 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98454, Requested 5468. Please try again in 56m28.553s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:28,927 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:28,927 - src.utils.helpers - INFO - LLM call for TATACONSUM took 6.11 seconds
2025-10-05 19:09:28,927 - src.methods.zero_shot - ERROR - Failed to process document 28: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98454, Requested 5468. Please try again in 56m28.553s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:28,927 - src.methods.zero_shot - INFO - Processing document 29/63: TATACONSUM
2025-10-05 19:09:28,927 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q3 2024 (Zero-shot)
2025-10-05 19:09:28,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:28,979 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98454, Requested 4398. Please try again in 41m4.022999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:28,979 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98454, Requested 4398. Please try again in 41m4.022999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:31,015 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:31,016 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98452, Requested 4398. Please try again in 41m1.981s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:31,017 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98452, Requested 4398. Please try again in 41m1.981s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:35,056 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:35,058 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98447, Requested 4398. Please try again in 40m57.94s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:35,059 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:35,059 - src.utils.helpers - INFO - LLM call for TATACONSUM took 6.13 seconds
2025-10-05 19:09:35,059 - src.methods.zero_shot - ERROR - Failed to process document 29: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98447, Requested 4398. Please try again in 40m57.94s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:35,060 - src.methods.zero_shot - INFO - Processing document 30/63: TATACONSUM
2025-10-05 19:09:35,061 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q3 2025 (Zero-shot)
2025-10-05 19:09:35,564 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-05 19:09:35,566 - src.utils.helpers - WARNING - Unknown model llama-3.3-70b-versatile, using default pricing
2025-10-05 19:09:35,566 - src.methods.zero_shot - INFO - API call completed in 0.51s, cost: $0.0019
2025-10-05 19:09:35,567 - src.utils.helpers - INFO - LLM call for TATACONSUM took 0.51 seconds
2025-10-05 19:09:35,570 - src.methods.zero_shot - INFO - Processing document 31/63: TATACONSUM
2025-10-05 19:09:35,570 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q4 2024 (Zero-shot)
2025-10-05 19:09:35,592 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:35,592 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99644, Requested 3295. Please try again in 42m19.267s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:35,592 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99644, Requested 3295. Please try again in 42m19.267s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:37,628 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:37,629 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99642, Requested 3295. Please try again in 42m17.234s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:37,630 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99642, Requested 3295. Please try again in 42m17.234s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:41,657 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:41,659 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99637, Requested 3295. Please try again in 42m13.205s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:41,660 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:41,660 - src.utils.helpers - INFO - LLM call for TATACONSUM took 6.09 seconds
2025-10-05 19:09:41,661 - src.methods.zero_shot - ERROR - Failed to process document 31: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99637, Requested 3295. Please try again in 42m13.205s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:41,662 - src.methods.zero_shot - INFO - Processing document 32/63: TATACONSUM
2025-10-05 19:09:41,662 - src.methods.zero_shot - INFO - Extracting metrics for TATACONSUM Q4 2025 (Zero-shot)
2025-10-05 19:09:41,663 - src.methods.zero_shot - WARNING - Truncating text from 66217 to 20000 chars
2025-10-05 19:09:41,697 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:41,698 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99637, Requested 5471. Please try again in 1h13m33.229s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:41,699 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99637, Requested 5471. Please try again in 1h13m33.229s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:43,744 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:43,745 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99635, Requested 5471. Please try again in 1h13m31.186s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:43,746 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99635, Requested 5471. Please try again in 1h13m31.186s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:48,142 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:48,143 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99630, Requested 5471. Please try again in 1h13m26.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:48,144 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:48,144 - src.utils.helpers - INFO - LLM call for TATACONSUM took 6.48 seconds
2025-10-05 19:09:48,145 - src.methods.zero_shot - ERROR - Failed to process document 32: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99630, Requested 5471. Please try again in 1h13m26.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:48,145 - src.methods.zero_shot - INFO - Processing document 33/63: BRITANNIA
2025-10-05 19:09:48,146 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q1 2024 (Zero-shot)
2025-10-05 19:09:48,146 - src.methods.zero_shot - WARNING - Truncating text from 45109 to 20000 chars
2025-10-05 19:09:48,187 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:48,188 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99630, Requested 5329. Please try again in 1h11m24.049999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:48,189 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99630, Requested 5329. Please try again in 1h11m24.049999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:50,222 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:50,222 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99628, Requested 5329. Please try again in 1h11m22.011999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:50,222 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99628, Requested 5329. Please try again in 1h11m22.011999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:09:54,243 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:54,243 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99623, Requested 5329. Please try again in 1h11m17.982999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:54,243 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:09:54,243 - src.methods.zero_shot - ERROR - Failed to process document 33: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99623, Requested 5329. Please try again in 1h11m17.982999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:54,243 - src.methods.zero_shot - INFO - Processing document 34/63: BRITANNIA
2025-10-05 19:09:54,243 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q1 2025 (Zero-shot)
2025-10-05 19:09:54,243 - src.methods.zero_shot - WARNING - Truncating text from 49804 to 20000 chars
2025-10-05 19:09:54,295 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:54,296 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99623, Requested 5330. Please try again in 1h11m18.805999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:54,296 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99623, Requested 5330. Please try again in 1h11m18.805999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:09:56,330 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:09:56,331 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99620, Requested 5330. Please try again in 1h11m16.770999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:09:56,332 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99620, Requested 5330. Please try again in 1h11m16.770999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:00,385 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:00,386 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99616, Requested 5330. Please try again in 1h11m12.734s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:00,387 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:00,387 - src.methods.zero_shot - ERROR - Failed to process document 34: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99616, Requested 5330. Please try again in 1h11m12.734s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:00,388 - src.methods.zero_shot - INFO - Processing document 35/63: BRITANNIA
2025-10-05 19:10:00,388 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q2 2024 (Zero-shot)
2025-10-05 19:10:00,389 - src.methods.zero_shot - WARNING - Truncating text from 46322 to 20000 chars
2025-10-05 19:10:00,422 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:00,425 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99616, Requested 5328. Please try again in 1h11m10.952999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:00,426 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99616, Requested 5328. Please try again in 1h11m10.952999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:02,446 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:02,446 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99613, Requested 5328. Please try again in 1h11m8.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:02,446 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99613, Requested 5328. Please try again in 1h11m8.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:06,523 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:06,523 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99609, Requested 5328. Please try again in 1h11m4.843s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:06,523 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:06,523 - src.methods.zero_shot - ERROR - Failed to process document 35: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99609, Requested 5328. Please try again in 1h11m4.843s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:06,523 - src.methods.zero_shot - INFO - Processing document 36/63: BRITANNIA
2025-10-05 19:10:06,523 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q2 2025 (Zero-shot)
2025-10-05 19:10:06,523 - src.methods.zero_shot - WARNING - Truncating text from 45706 to 20000 chars
2025-10-05 19:10:06,555 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:06,555 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99609, Requested 5331. Please try again in 1h11m7.402s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:06,565 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99609, Requested 5331. Please try again in 1h11m7.402s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:08,589 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:08,589 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99606, Requested 5331. Please try again in 1h11m5.367999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:08,589 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99606, Requested 5331. Please try again in 1h11m5.367999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:12,617 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:12,617 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99602, Requested 5331. Please try again in 1h11m1.337999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:12,617 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:12,617 - src.methods.zero_shot - ERROR - Failed to process document 36: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99602, Requested 5331. Please try again in 1h11m1.337999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:12,617 - src.methods.zero_shot - INFO - Processing document 37/63: BRITANNIA
2025-10-05 19:10:12,617 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q3 2024 (Zero-shot)
2025-10-05 19:10:12,617 - src.methods.zero_shot - WARNING - Truncating text from 49047 to 20000 chars
2025-10-05 19:10:12,663 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:12,663 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99602, Requested 5327. Please try again in 1h10m57.844s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:12,663 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99602, Requested 5327. Please try again in 1h10m57.844s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:14,689 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:14,690 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99599, Requested 5327. Please try again in 1h10m55.821s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:14,690 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99599, Requested 5327. Please try again in 1h10m55.821s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:18,727 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:18,727 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99595, Requested 5327. Please try again in 1h10m51.781s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:18,727 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:18,727 - src.methods.zero_shot - ERROR - Failed to process document 37: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99595, Requested 5327. Please try again in 1h10m51.781s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:18,727 - src.methods.zero_shot - INFO - Processing document 38/63: BRITANNIA
2025-10-05 19:10:18,727 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q3 2025 (Zero-shot)
2025-10-05 19:10:18,727 - src.methods.zero_shot - WARNING - Truncating text from 45521 to 20000 chars
2025-10-05 19:10:18,764 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:18,768 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99594, Requested 5330. Please try again in 1h10m54.336s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:18,768 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99594, Requested 5330. Please try again in 1h10m54.336s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:20,816 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:20,817 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99592, Requested 5330. Please try again in 1h10m52.285s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:20,818 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99592, Requested 5330. Please try again in 1h10m52.285s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:24,845 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:24,846 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99587, Requested 5330. Please try again in 1h10m48.256999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:24,846 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:24,847 - src.methods.zero_shot - ERROR - Failed to process document 38: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99587, Requested 5330. Please try again in 1h10m48.256999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:24,847 - src.methods.zero_shot - INFO - Processing document 39/63: BRITANNIA
2025-10-05 19:10:24,847 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q4 2024 (Zero-shot)
2025-10-05 19:10:24,848 - src.methods.zero_shot - WARNING - Truncating text from 45461 to 20000 chars
2025-10-05 19:10:24,874 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:24,874 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99587, Requested 5327. Please try again in 1h10m45.635999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:24,874 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99587, Requested 5327. Please try again in 1h10m45.635999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:26,918 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:26,919 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99585, Requested 5327. Please try again in 1h10m43.590999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:26,919 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99585, Requested 5327. Please try again in 1h10m43.590999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:30,971 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:30,972 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99580, Requested 5327. Please try again in 1h10m39.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:30,973 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:30,973 - src.methods.zero_shot - ERROR - Failed to process document 39: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99580, Requested 5327. Please try again in 1h10m39.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:30,973 - src.methods.zero_shot - INFO - Processing document 40/63: BRITANNIA
2025-10-05 19:10:30,973 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q4 2025 (Zero-shot)
2025-10-05 19:10:30,974 - src.methods.zero_shot - WARNING - Truncating text from 47802 to 20000 chars
2025-10-05 19:10:31,005 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:31,006 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99580, Requested 5331. Please try again in 1h10m42.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:31,006 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99580, Requested 5331. Please try again in 1h10m42.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:33,023 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:33,023 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99578, Requested 5331. Please try again in 1h10m40.930999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:33,023 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99578, Requested 5331. Please try again in 1h10m40.930999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:37,053 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:37,054 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 5331. Please try again in 1h10m36.911999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:37,055 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:37,055 - src.methods.zero_shot - ERROR - Failed to process document 40: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 5331. Please try again in 1h10m36.911999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:37,055 - src.methods.zero_shot - INFO - Processing document 41/63: DABUR
2025-10-05 19:10:37,055 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q1 2024 (Zero-shot)
2025-10-05 19:10:37,055 - src.methods.zero_shot - WARNING - Truncating text from 61085 to 20000 chars
2025-10-05 19:10:37,092 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:37,093 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 5320. Please try again in 1h10m27.372999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:37,093 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99573, Requested 5320. Please try again in 1h10m27.372999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:39,117 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:39,117 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99571, Requested 5320. Please try again in 1h10m25.332999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:39,117 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99571, Requested 5320. Please try again in 1h10m25.332999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:43,165 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:43,165 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99566, Requested 5320. Please try again in 1h10m21.287999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:43,165 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:43,165 - src.methods.zero_shot - ERROR - Failed to process document 41: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99566, Requested 5320. Please try again in 1h10m21.287999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:43,165 - src.methods.zero_shot - INFO - Processing document 42/63: DABUR
2025-10-05 19:10:43,165 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q1 2025 (Zero-shot)
2025-10-05 19:10:43,165 - src.methods.zero_shot - WARNING - Truncating text from 58416 to 20000 chars
2025-10-05 19:10:43,209 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:43,210 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99566, Requested 5325. Please try again in 1h10m25.571999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:43,211 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99566, Requested 5325. Please try again in 1h10m25.571999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:45,246 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:45,247 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99564, Requested 5325. Please try again in 1h10m23.535s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:45,248 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99564, Requested 5325. Please try again in 1h10m23.535s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:49,287 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:49,287 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99559, Requested 5325. Please try again in 1h10m19.495s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:49,287 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:49,287 - src.methods.zero_shot - ERROR - Failed to process document 42: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99559, Requested 5325. Please try again in 1h10m19.495s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:49,287 - src.methods.zero_shot - INFO - Processing document 43/63: DABUR
2025-10-05 19:10:49,287 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q2 2024 (Zero-shot)
2025-10-05 19:10:49,287 - src.methods.zero_shot - WARNING - Truncating text from 37834 to 20000 chars
2025-10-05 19:10:49,321 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:49,321 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99559, Requested 5319. Please try again in 1h10m14.277s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:49,321 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99559, Requested 5319. Please try again in 1h10m14.277s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:51,365 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:51,367 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99557, Requested 5319. Please try again in 1h10m12.233s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:51,367 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99557, Requested 5319. Please try again in 1h10m12.233s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:10:55,399 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:55,400 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99552, Requested 5319. Please try again in 1h10m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:55,401 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:10:55,401 - src.methods.zero_shot - ERROR - Failed to process document 43: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99552, Requested 5319. Please try again in 1h10m8.198s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:55,402 - src.methods.zero_shot - INFO - Processing document 44/63: DABUR
2025-10-05 19:10:55,402 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q2 2025 (Zero-shot)
2025-10-05 19:10:55,403 - src.methods.zero_shot - WARNING - Truncating text from 68411 to 20000 chars
2025-10-05 19:10:55,465 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:55,465 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99552, Requested 5321. Please try again in 1h10m9.857999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:55,465 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99552, Requested 5321. Please try again in 1h10m9.857999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:10:57,508 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:10:57,508 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99550, Requested 5321. Please try again in 1h10m7.812s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:10:57,508 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99550, Requested 5321. Please try again in 1h10m7.812s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:01,531 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:01,531 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99545, Requested 5321. Please try again in 1h10m3.781s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:01,546 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:01,547 - src.methods.zero_shot - ERROR - Failed to process document 44: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99545, Requested 5321. Please try again in 1h10m3.781s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:01,548 - src.methods.zero_shot - INFO - Processing document 45/63: DABUR
2025-10-05 19:11:01,549 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q3 2024 (Zero-shot)
2025-10-05 19:11:01,549 - src.methods.zero_shot - WARNING - Truncating text from 51644 to 20000 chars
2025-10-05 19:11:01,582 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:01,583 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99545, Requested 5324. Please try again in 1h10m6.335s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:01,585 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99545, Requested 5324. Please try again in 1h10m6.335s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:03,611 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:03,611 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99543, Requested 5324. Please try again in 1h10m4.294s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:03,611 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99543, Requested 5324. Please try again in 1h10m4.294s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:07,651 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:07,652 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99538, Requested 5324. Please try again in 1h10m0.266s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:07,653 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:07,653 - src.methods.zero_shot - ERROR - Failed to process document 45: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99538, Requested 5324. Please try again in 1h10m0.266s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:07,654 - src.methods.zero_shot - INFO - Processing document 46/63: DABUR
2025-10-05 19:11:07,654 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q3 2025 (Zero-shot)
2025-10-05 19:11:07,654 - src.methods.zero_shot - WARNING - Truncating text from 62347 to 20000 chars
2025-10-05 19:11:07,687 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:07,688 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99538, Requested 5320. Please try again in 1h9m56.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:07,689 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99538, Requested 5320. Please try again in 1h9m56.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:09,728 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:09,729 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99536, Requested 5320. Please try again in 1h9m54.733999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:09,729 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99536, Requested 5320. Please try again in 1h9m54.733999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:13,774 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:13,774 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99531, Requested 5320. Please try again in 1h9m50.685999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:13,774 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:13,774 - src.methods.zero_shot - ERROR - Failed to process document 46: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99531, Requested 5320. Please try again in 1h9m50.685999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:13,774 - src.methods.zero_shot - INFO - Processing document 47/63: DABUR
2025-10-05 19:11:13,774 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q4 2024 (Zero-shot)
2025-10-05 19:11:13,774 - src.methods.zero_shot - WARNING - Truncating text from 74312 to 20000 chars
2025-10-05 19:11:13,814 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:13,815 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99531, Requested 5319. Please try again in 1h9m49.782999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:13,817 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99531, Requested 5319. Please try again in 1h9m49.782999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:15,845 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:15,846 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99528, Requested 5319. Please try again in 1h9m47.751999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:15,847 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99528, Requested 5319. Please try again in 1h9m47.751999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:19,879 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:19,880 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99524, Requested 5319. Please try again in 1h9m43.716999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:19,881 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:19,881 - src.methods.zero_shot - ERROR - Failed to process document 47: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99524, Requested 5319. Please try again in 1h9m43.716999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:19,882 - src.methods.zero_shot - INFO - Processing document 48/63: DABUR
2025-10-05 19:11:19,882 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q4 2025 (Zero-shot)
2025-10-05 19:11:19,883 - src.methods.zero_shot - WARNING - Truncating text from 46453 to 20000 chars
2025-10-05 19:11:19,908 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:19,909 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99524, Requested 5319. Please try again in 1h9m43.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:19,910 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99524, Requested 5319. Please try again in 1h9m43.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:21,962 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:21,963 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99521, Requested 5319. Please try again in 1h9m41.634999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:21,963 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99521, Requested 5319. Please try again in 1h9m41.634999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:26,023 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:26,024 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99517, Requested 5319. Please try again in 1h9m37.575s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:26,025 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:26,025 - src.methods.zero_shot - ERROR - Failed to process document 48: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99517, Requested 5319. Please try again in 1h9m37.575s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:26,026 - src.methods.zero_shot - INFO - Processing document 49/63: MARICO
2025-10-05 19:11:26,026 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q1 2024 (Zero-shot)
2025-10-05 19:11:26,026 - src.methods.zero_shot - WARNING - Truncating text from 52196 to 20000 chars
2025-10-05 19:11:26,064 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:26,066 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99517, Requested 5331. Please try again in 1h9m47.902s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:26,070 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99517, Requested 5331. Please try again in 1h9m47.902s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:28,102 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:28,102 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99514, Requested 5331. Please try again in 1h9m45.854s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:28,102 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99514, Requested 5331. Please try again in 1h9m45.854s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:32,137 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:32,137 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99510, Requested 5331. Please try again in 1h9m41.827s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:32,138 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:32,138 - src.methods.zero_shot - ERROR - Failed to process document 49: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99510, Requested 5331. Please try again in 1h9m41.827s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:32,139 - src.methods.zero_shot - INFO - Processing document 50/63: MARICO
2025-10-05 19:11:32,139 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q2 2024 (Zero-shot)
2025-10-05 19:11:32,139 - src.methods.zero_shot - WARNING - Truncating text from 47370 to 20000 chars
2025-10-05 19:11:32,164 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:32,165 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99510, Requested 5328. Please try again in 1h9m39.209s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:32,166 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99510, Requested 5328. Please try again in 1h9m39.209s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:34,209 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:34,210 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99507, Requested 5328. Please try again in 1h9m37.169s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:34,210 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99507, Requested 5328. Please try again in 1h9m37.169s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:38,253 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:38,254 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99502, Requested 5328. Please try again in 1h9m33.119s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:38,255 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:38,255 - src.methods.zero_shot - ERROR - Failed to process document 50: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99502, Requested 5328. Please try again in 1h9m33.119s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:38,256 - src.methods.zero_shot - INFO - Processing document 51/63: MARICO
2025-10-05 19:11:38,256 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q2 2025 (Zero-shot)
2025-10-05 19:11:38,257 - src.methods.zero_shot - WARNING - Truncating text from 60858 to 20000 chars
2025-10-05 19:11:38,294 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:38,295 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99502, Requested 5329. Please try again in 1h9m33.943999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:38,296 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99502, Requested 5329. Please try again in 1h9m33.943999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:40,321 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:40,321 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99500, Requested 5329. Please try again in 1h9m31.907s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:40,321 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99500, Requested 5329. Please try again in 1h9m31.907s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:44,354 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:44,355 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99495, Requested 5329. Please try again in 1h9m27.883s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:44,356 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:44,356 - src.methods.zero_shot - ERROR - Failed to process document 51: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99495, Requested 5329. Please try again in 1h9m27.883s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:44,357 - src.methods.zero_shot - INFO - Processing document 52/63: MARICO
2025-10-05 19:11:44,357 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q3 2024 (Zero-shot)
2025-10-05 19:11:44,357 - src.methods.zero_shot - WARNING - Truncating text from 58502 to 20000 chars
2025-10-05 19:11:44,393 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:44,395 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99495, Requested 5329. Please try again in 1h9m27.846999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:44,396 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99495, Requested 5329. Please try again in 1h9m27.846999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:46,434 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:46,435 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99493, Requested 5329. Please try again in 1h9m25.803s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:46,436 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99493, Requested 5329. Please try again in 1h9m25.803s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:50,465 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:50,467 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99488, Requested 5329. Please try again in 1h9m21.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:50,467 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:50,468 - src.methods.zero_shot - ERROR - Failed to process document 52: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99488, Requested 5329. Please try again in 1h9m21.772s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:50,469 - src.methods.zero_shot - INFO - Processing document 53/63: MARICO
2025-10-05 19:11:50,469 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q3 2025 (Zero-shot)
2025-10-05 19:11:50,470 - src.methods.zero_shot - WARNING - Truncating text from 25546 to 20000 chars
2025-10-05 19:11:50,496 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:50,497 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99488, Requested 5332. Please try again in 1h9m24.332999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:50,498 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99488, Requested 5332. Please try again in 1h9m24.332999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:52,532 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:52,534 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99486, Requested 5332. Please try again in 1h9m22.296s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:52,534 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99486, Requested 5332. Please try again in 1h9m22.296s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:11:56,561 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:56,562 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99481, Requested 5332. Please try again in 1h9m18.267s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:56,563 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:11:56,563 - src.methods.zero_shot - ERROR - Failed to process document 53: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99481, Requested 5332. Please try again in 1h9m18.267s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:56,564 - src.methods.zero_shot - INFO - Processing document 54/63: MARICO
2025-10-05 19:11:56,564 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q4 2024 (Zero-shot)
2025-10-05 19:11:56,565 - src.methods.zero_shot - WARNING - Truncating text from 54547 to 20000 chars
2025-10-05 19:11:56,612 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:56,613 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99481, Requested 5335. Please try again in 1h9m20.808s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:56,614 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99481, Requested 5335. Please try again in 1h9m20.808s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:11:58,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:11:58,726 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99479, Requested 5335. Please try again in 1h9m18.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:11:58,727 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99479, Requested 5335. Please try again in 1h9m18.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:02,759 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:02,760 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99474, Requested 5335. Please try again in 1h9m14.661s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:02,761 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:02,761 - src.methods.zero_shot - ERROR - Failed to process document 54: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99474, Requested 5335. Please try again in 1h9m14.661s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:02,761 - src.methods.zero_shot - INFO - Processing document 55/63: MARICO
2025-10-05 19:12:02,761 - src.methods.zero_shot - INFO - Extracting commentary for MARICO Q4 2025 (Zero-shot)
2025-10-05 19:12:02,761 - src.methods.zero_shot - WARNING - Truncating text from 59331 to 20000 chars
2025-10-05 19:12:02,789 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:02,790 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99474, Requested 5328. Please try again in 1h9m8.584s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:02,791 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99474, Requested 5328. Please try again in 1h9m8.584s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:04,823 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:04,824 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99472, Requested 5328. Please try again in 1h9m6.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:04,825 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99472, Requested 5328. Please try again in 1h9m6.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:08,870 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:08,871 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99467, Requested 5328. Please try again in 1h9m2.509s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:08,871 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:08,872 - src.methods.zero_shot - ERROR - Failed to process document 55: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99467, Requested 5328. Please try again in 1h9m2.509s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:08,873 - src.methods.zero_shot - INFO - Processing document 56/63: TATACONSUM
2025-10-05 19:12:08,874 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q1 2024 (Zero-shot)
2025-10-05 19:12:08,874 - src.methods.zero_shot - WARNING - Truncating text from 57373 to 20000 chars
2025-10-05 19:12:08,908 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:08,910 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99467, Requested 5330. Please try again in 1h9m4.193s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:08,910 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99467, Requested 5330. Please try again in 1h9m4.193s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:10,938 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:10,940 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99465, Requested 5330. Please try again in 1h9m2.162s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:10,941 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99465, Requested 5330. Please try again in 1h9m2.162s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:14,962 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:14,962 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99460, Requested 5330. Please try again in 1h8m58.129s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:14,962 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:14,962 - src.methods.zero_shot - ERROR - Failed to process document 56: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99460, Requested 5330. Please try again in 1h8m58.129s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:14,962 - src.methods.zero_shot - INFO - Processing document 57/63: TATACONSUM
2025-10-05 19:12:14,962 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q1 2025 (Zero-shot)
2025-10-05 19:12:14,962 - src.methods.zero_shot - WARNING - Truncating text from 46463 to 20000 chars
2025-10-05 19:12:15,008 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:15,009 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99460, Requested 5331. Please try again in 1h8m58.957s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:15,009 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99460, Requested 5331. Please try again in 1h8m58.957s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:17,061 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:17,062 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99458, Requested 5331. Please try again in 1h8m56.907s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:17,063 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99458, Requested 5331. Please try again in 1h8m56.907s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:21,103 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:21,104 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99453, Requested 5331. Please try again in 1h8m52.862s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:21,104 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:21,105 - src.methods.zero_shot - ERROR - Failed to process document 57: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99453, Requested 5331. Please try again in 1h8m52.862s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:21,105 - src.methods.zero_shot - INFO - Processing document 58/63: TATACONSUM
2025-10-05 19:12:21,106 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q2 2024 (Zero-shot)
2025-10-05 19:12:21,106 - src.methods.zero_shot - WARNING - Truncating text from 55717 to 20000 chars
2025-10-05 19:12:21,137 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:21,139 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99453, Requested 5336. Please try again in 1h8m57.148s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:21,140 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99453, Requested 5336. Please try again in 1h8m57.148s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:23,194 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:23,195 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99450, Requested 5336. Please try again in 1h8m55.09s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:23,195 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99450, Requested 5336. Please try again in 1h8m55.09s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:27,248 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:27,250 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99446, Requested 5336. Please try again in 1h8m51.038s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:27,250 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:27,250 - src.methods.zero_shot - ERROR - Failed to process document 58: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99446, Requested 5336. Please try again in 1h8m51.038s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:27,250 - src.methods.zero_shot - INFO - Processing document 59/63: TATACONSUM
2025-10-05 19:12:27,253 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q2 2025 (Zero-shot)
2025-10-05 19:12:27,255 - src.methods.zero_shot - WARNING - Truncating text from 54849 to 20000 chars
2025-10-05 19:12:27,284 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:27,288 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99446, Requested 5328. Please try again in 1h8m44.088999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:27,288 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99446, Requested 5328. Please try again in 1h8m44.088999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:29,329 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:29,329 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99443, Requested 5328. Please try again in 1h8m42.044s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:29,329 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99443, Requested 5328. Please try again in 1h8m42.044s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:33,383 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:33,384 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99439, Requested 5328. Please try again in 1h8m37.99s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:33,385 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:33,385 - src.methods.zero_shot - ERROR - Failed to process document 59: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99439, Requested 5328. Please try again in 1h8m37.99s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:33,386 - src.methods.zero_shot - INFO - Processing document 60/63: TATACONSUM
2025-10-05 19:12:33,386 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q3 2024 (Zero-shot)
2025-10-05 19:12:33,386 - src.methods.zero_shot - WARNING - Truncating text from 54530 to 20000 chars
2025-10-05 19:12:33,420 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:33,422 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99439, Requested 5329. Please try again in 1h8m38.817999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:33,423 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99439, Requested 5329. Please try again in 1h8m38.817999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:35,475 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:35,476 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99436, Requested 5329. Please try again in 1h8m36.761s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:35,477 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99436, Requested 5329. Please try again in 1h8m36.761s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:39,511 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:39,511 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99432, Requested 5329. Please try again in 1h8m32.726s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:39,511 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:39,511 - src.methods.zero_shot - ERROR - Failed to process document 60: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99432, Requested 5329. Please try again in 1h8m32.726s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:39,512 - src.methods.zero_shot - INFO - Processing document 61/63: TATACONSUM
2025-10-05 19:12:39,512 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q3 2025 (Zero-shot)
2025-10-05 19:12:39,513 - src.methods.zero_shot - WARNING - Truncating text from 51341 to 20000 chars
2025-10-05 19:12:39,543 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:39,544 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99432, Requested 5331. Please try again in 1h8m34.421s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:39,545 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99432, Requested 5331. Please try again in 1h8m34.421s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:41,574 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:41,576 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99429, Requested 5331. Please try again in 1h8m32.389999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:41,577 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99429, Requested 5331. Please try again in 1h8m32.389999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:45,850 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:45,850 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99425, Requested 5331. Please try again in 1h8m28.358s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:45,850 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:45,850 - src.methods.zero_shot - ERROR - Failed to process document 61: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99425, Requested 5331. Please try again in 1h8m28.358s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:45,850 - src.methods.zero_shot - INFO - Processing document 62/63: TATACONSUM
2025-10-05 19:12:45,850 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q4 2024 (Zero-shot)
2025-10-05 19:12:45,850 - src.methods.zero_shot - WARNING - Truncating text from 43737 to 20000 chars
2025-10-05 19:12:45,888 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:45,888 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99424, Requested 5335. Please try again in 1h8m31.528999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:45,888 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99424, Requested 5335. Please try again in 1h8m31.528999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:47,914 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:47,914 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99422, Requested 5335. Please try again in 1h8m29.492999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:47,914 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99422, Requested 5335. Please try again in 1h8m29.492999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:51,961 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:51,961 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99417, Requested 5335. Please try again in 1h8m25.446s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:51,977 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:51,977 - src.methods.zero_shot - ERROR - Failed to process document 62: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99417, Requested 5335. Please try again in 1h8m25.446s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:51,977 - src.methods.zero_shot - INFO - Processing document 63/63: TATACONSUM
2025-10-05 19:12:51,977 - src.methods.zero_shot - INFO - Extracting commentary for TATACONSUM Q4 2025 (Zero-shot)
2025-10-05 19:12:51,977 - src.methods.zero_shot - WARNING - Truncating text from 49027 to 20000 chars
2025-10-05 19:12:52,003 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:52,004 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99417, Requested 5330. Please try again in 1h8m21.095999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:52,004 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99417, Requested 5330. Please try again in 1h8m21.095999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-05 19:12:54,039 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:54,039 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99415, Requested 5330. Please try again in 1h8m19.060999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:54,039 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99415, Requested 5330. Please try again in 1h8m19.060999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-05 19:12:58,078 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-05 19:12:58,079 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99410, Requested 5330. Please try again in 1h8m15.022s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:58,080 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-05 19:12:58,080 - src.methods.zero_shot - ERROR - Failed to process document 63: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k4mm6yhmew28zy3z70732w3s` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99410, Requested 5330. Please try again in 1h8m15.022s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-05 19:12:58,083 - src.methods.zero_shot - INFO - Batch extraction complete. Total cost: $0.1526
2025-10-05 19:12:58,084 - experiment.financial_rag_pipeline - INFO - Metrics:
2025-10-05 19:12:58,087 - experiment.financial_rag_pipeline - INFO -   zero_shot_model: llama-3.3-70b-versatile
2025-10-05 19:12:58,087 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_cost: 0.15259499999999998
2025-10-05 19:12:58,088 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_tokens: 100210
2025-10-05 19:12:58,088 - experiment.financial_rag_pipeline - INFO -   zero_shot_num_requests: 17
2025-10-05 19:12:58,089 - src.utils.helpers - INFO - run_zero_shot_extraction took 704.54 seconds
2025-10-12 12:06:18,907 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-12 12:06:22,057 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-10-12 12:06:40,214 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-12 12:06:40,214 - src.utils.helpers - INFO - _initialize_model took 21.31 seconds
2025-10-12 12:06:40,255 - src.data_preprocessing.embed_store - INFO - Loaded FAISS index from data\embeddings\financial_index.faiss
2025-10-12 12:06:40,274 - src.utils.resume_extraction - INFO - RAG Resume Stats: 0/63 completed
2025-10-12 12:06:40,275 - src.utils.resume_extraction - INFO - Found 0 processed documents, 63 remaining to process
2025-10-12 12:06:40,275 - src.utils.resume_extraction - INFO - Resuming RAG extraction for 63 documents...
2025-10-12 12:06:40,276 - src.methods.rag - INFO - Starting RAG batch extraction for 63 documents
2025-10-12 12:06:40,276 - src.methods.rag - INFO - Processing document 1/63 (RAG): BRITANNIA
2025-10-12 12:06:40,276 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q1 2024 (RAG)
2025-10-12 12:06:40,377 - src.utils.helpers - INFO - generate_embeddings took 0.10 seconds
2025-10-12 12:06:40,379 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:40,379 - src.methods.rag - INFO - Retrieved 2 contexts in 0.10s for: Extract all financial metrics...
2025-10-12 12:06:41,672 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:41,687 - src.methods.rag - INFO - RAG API call completed in 1.31s, cost: $0.0014
2025-10-12 12:06:41,701 - src.methods.rag - INFO - Processing document 2/63 (RAG): BRITANNIA
2025-10-12 12:06:41,702 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q1 2025 (RAG)
2025-10-12 12:06:41,713 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:06:41,722 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:06:41,722 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:42,709 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:42,710 - src.methods.rag - INFO - RAG API call completed in 0.99s, cost: $0.0011
2025-10-12 12:06:42,712 - src.methods.rag - INFO - Processing document 3/63 (RAG): BRITANNIA
2025-10-12 12:06:42,713 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q2 2024 (RAG)
2025-10-12 12:06:42,730 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:06:42,732 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:42,732 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:43,684 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:43,700 - src.methods.rag - INFO - RAG API call completed in 0.97s, cost: $0.0011
2025-10-12 12:06:43,700 - src.methods.rag - INFO - Processing document 4/63 (RAG): BRITANNIA
2025-10-12 12:06:43,700 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q2 2025 (RAG)
2025-10-12 12:06:43,710 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:06:43,721 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:43,722 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:43,722 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:06:44,700 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:44,700 - src.methods.rag - INFO - RAG API call completed in 0.98s, cost: $0.0009
2025-10-12 12:06:44,700 - src.methods.rag - INFO - Processing document 5/63 (RAG): BRITANNIA
2025-10-12 12:06:44,700 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q3 2024 (RAG)
2025-10-12 12:06:44,722 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:06:44,730 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:06:44,731 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:06:44,731 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:06:45,706 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:45,706 - src.methods.rag - INFO - RAG API call completed in 0.97s, cost: $0.0009
2025-10-12 12:06:45,706 - src.methods.rag - INFO - Processing document 6/63 (RAG): BRITANNIA
2025-10-12 12:06:45,706 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q3 2025 (RAG)
2025-10-12 12:06:45,726 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:06:45,729 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:45,730 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:47,434 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:47,434 - src.methods.rag - INFO - RAG API call completed in 1.70s, cost: $0.0011
2025-10-12 12:06:47,434 - src.methods.rag - ERROR - Failed to parse RAG JSON response: Expecting property name enclosed in double quotes: line 12 column 24 (char 284)
2025-10-12 12:06:47,450 - src.methods.rag - INFO - Processing document 7/63 (RAG): BRITANNIA
2025-10-12 12:06:47,450 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q4 2024 (RAG)
2025-10-12 12:06:47,450 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:06:47,467 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:06:47,467 - src.methods.rag - INFO - Retrieved 2 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:48,537 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:48,537 - src.methods.rag - INFO - RAG API call completed in 1.07s, cost: $0.0014
2025-10-12 12:06:48,537 - src.methods.rag - INFO - Processing document 8/63 (RAG): BRITANNIA
2025-10-12 12:06:48,537 - src.methods.rag - INFO - Extracting metrics for BRITANNIA Q4 2025 (RAG)
2025-10-12 12:06:48,669 - src.utils.helpers - INFO - generate_embeddings took 0.13 seconds
2025-10-12 12:06:48,678 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:06:48,679 - src.methods.rag - INFO - Retrieved 0 contexts in 0.14s for: Extract all financial metrics...
2025-10-12 12:06:48,679 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:06:49,656 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:49,656 - src.methods.rag - INFO - RAG API call completed in 0.98s, cost: $0.0009
2025-10-12 12:06:49,656 - src.methods.rag - INFO - Processing document 9/63 (RAG): DABUR
2025-10-12 12:06:49,656 - src.methods.rag - INFO - Extracting metrics for DABUR Q1 2024 (RAG)
2025-10-12 12:06:49,675 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:06:49,678 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:49,678 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:49,678 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:06:49,708 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:06:49,709 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:06:54,742 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:06:54,742 - src.methods.rag - INFO - RAG API call completed in 5.06s, cost: $0.0009
2025-10-12 12:06:54,742 - src.methods.rag - INFO - Processing document 10/63 (RAG): DABUR
2025-10-12 12:06:54,742 - src.methods.rag - INFO - Extracting metrics for DABUR Q1 2025 (RAG)
2025-10-12 12:06:54,760 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:06:54,763 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:06:54,763 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:06:54,764 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:06:54,803 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:06:54,803 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:07:00,794 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:00,794 - src.methods.rag - INFO - RAG API call completed in 6.03s, cost: $0.0009
2025-10-12 12:07:00,794 - src.methods.rag - INFO - Processing document 11/63 (RAG): DABUR
2025-10-12 12:07:00,794 - src.methods.rag - INFO - Extracting metrics for DABUR Q2 2024 (RAG)
2025-10-12 12:07:00,819 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:00,819 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:00,819 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:00,848 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:00,849 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 8.000000 seconds
2025-10-12 12:07:09,834 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:09,835 - src.methods.rag - INFO - RAG API call completed in 9.02s, cost: $0.0011
2025-10-12 12:07:09,837 - src.methods.rag - INFO - Processing document 12/63 (RAG): DABUR
2025-10-12 12:07:09,838 - src.methods.rag - INFO - Extracting metrics for DABUR Q2 2025 (RAG)
2025-10-12 12:07:09,853 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:09,856 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:09,856 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:09,857 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:09,902 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:09,903 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:07:16,900 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:16,900 - src.methods.rag - INFO - RAG API call completed in 7.04s, cost: $0.0009
2025-10-12 12:07:16,900 - src.methods.rag - INFO - Processing document 13/63 (RAG): DABUR
2025-10-12 12:07:16,900 - src.methods.rag - INFO - Extracting metrics for DABUR Q3 2024 (RAG)
2025-10-12 12:07:16,921 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:16,924 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:16,924 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:16,924 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:16,966 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:16,967 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:07:23,946 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:23,946 - src.methods.rag - INFO - RAG API call completed in 7.02s, cost: $0.0009
2025-10-12 12:07:23,961 - src.methods.rag - INFO - Processing document 14/63 (RAG): DABUR
2025-10-12 12:07:23,961 - src.methods.rag - INFO - Extracting metrics for DABUR Q3 2025 (RAG)
2025-10-12 12:07:23,961 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:07:23,979 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:23,979 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:23,979 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:23,999 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:23,999 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:07:29,955 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:29,955 - src.methods.rag - INFO - RAG API call completed in 5.97s, cost: $0.0009
2025-10-12 12:07:29,955 - src.methods.rag - INFO - Processing document 15/63 (RAG): DABUR
2025-10-12 12:07:29,955 - src.methods.rag - INFO - Extracting metrics for DABUR Q4 2024 (RAG)
2025-10-12 12:07:29,982 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:07:29,982 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:29,982 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:07:29,982 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:30,019 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:30,020 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:07:37,039 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:37,039 - src.methods.rag - INFO - RAG API call completed in 7.05s, cost: $0.0009
2025-10-12 12:07:37,039 - src.methods.rag - INFO - Processing document 16/63 (RAG): DABUR
2025-10-12 12:07:37,039 - src.methods.rag - INFO - Extracting metrics for DABUR Q4 2025 (RAG)
2025-10-12 12:07:37,057 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:37,057 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:37,057 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:37,057 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:37,086 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:37,086 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:07:44,203 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:44,203 - src.methods.rag - INFO - RAG API call completed in 7.15s, cost: $0.0009
2025-10-12 12:07:44,203 - src.methods.rag - INFO - Processing document 17/63 (RAG): MARICO
2025-10-12 12:07:44,203 - src.methods.rag - INFO - Extracting metrics for MARICO Q1 2024 (RAG)
2025-10-12 12:07:44,226 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:44,229 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:44,230 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:07:44,230 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:44,259 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:44,261 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:07:50,212 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:50,212 - src.methods.rag - INFO - RAG API call completed in 5.98s, cost: $0.0009
2025-10-12 12:07:50,212 - src.methods.rag - INFO - Processing document 18/63 (RAG): MARICO
2025-10-12 12:07:50,212 - src.methods.rag - INFO - Extracting metrics for MARICO Q1 2025 (RAG)
2025-10-12 12:07:50,235 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:07:50,240 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:50,240 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:07:50,241 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:50,277 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:50,278 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:07:57,256 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:07:57,272 - src.methods.rag - INFO - RAG API call completed in 7.03s, cost: $0.0009
2025-10-12 12:07:57,272 - src.methods.rag - INFO - Processing document 19/63 (RAG): MARICO
2025-10-12 12:07:57,272 - src.methods.rag - INFO - Extracting metrics for MARICO Q2 2024 (RAG)
2025-10-12 12:07:57,283 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:07:57,295 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:07:57,295 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:07:57,295 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:07:57,317 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:07:57,317 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:04,342 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:04,342 - src.methods.rag - INFO - RAG API call completed in 7.05s, cost: $0.0009
2025-10-12 12:08:04,342 - src.methods.rag - INFO - Processing document 20/63 (RAG): MARICO
2025-10-12 12:08:04,342 - src.methods.rag - INFO - Extracting metrics for MARICO Q2 2025 (RAG)
2025-10-12 12:08:04,364 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:08:04,366 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:04,367 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:04,367 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:04,410 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:04,410 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:11,438 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:11,438 - src.methods.rag - INFO - RAG API call completed in 7.07s, cost: $0.0009
2025-10-12 12:08:11,438 - src.methods.rag - INFO - Processing document 21/63 (RAG): MARICO
2025-10-12 12:08:11,438 - src.methods.rag - INFO - Extracting metrics for MARICO Q3 2024 (RAG)
2025-10-12 12:08:11,463 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:08:11,465 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:11,466 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:08:11,466 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:11,497 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:11,498 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:08:17,477 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:17,477 - src.methods.rag - INFO - RAG API call completed in 6.01s, cost: $0.0009
2025-10-12 12:08:17,493 - src.methods.rag - INFO - Processing document 22/63 (RAG): MARICO
2025-10-12 12:08:17,493 - src.methods.rag - INFO - Extracting metrics for MARICO Q3 2025 (RAG)
2025-10-12 12:08:17,493 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:08:17,509 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:08:17,509 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:17,509 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:17,531 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:17,531 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:24,503 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:24,503 - src.methods.rag - INFO - RAG API call completed in 6.99s, cost: $0.0009
2025-10-12 12:08:24,503 - src.methods.rag - INFO - Processing document 23/63 (RAG): MARICO
2025-10-12 12:08:24,519 - src.methods.rag - INFO - Extracting metrics for MARICO Q4 2024 (RAG)
2025-10-12 12:08:24,519 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:08:24,534 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:08:24,534 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:24,534 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:24,575 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:24,575 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:31,602 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:31,602 - src.methods.rag - INFO - RAG API call completed in 7.07s, cost: $0.0009
2025-10-12 12:08:31,618 - src.methods.rag - INFO - Processing document 24/63 (RAG): MARICO
2025-10-12 12:08:31,618 - src.methods.rag - INFO - Extracting metrics for MARICO Q4 2025 (RAG)
2025-10-12 12:08:31,618 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:08:31,635 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:31,635 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:31,635 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:31,657 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:31,657 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:08:37,650 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:37,650 - src.methods.rag - INFO - RAG API call completed in 6.01s, cost: $0.0009
2025-10-12 12:08:37,650 - src.methods.rag - INFO - Processing document 25/63 (RAG): TATACONSUM
2025-10-12 12:08:37,650 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q1 2024 (RAG)
2025-10-12 12:08:37,665 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:08:37,679 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:08:37,679 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:08:37,681 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:37,709 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:37,709 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:44,826 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:44,826 - src.methods.rag - INFO - RAG API call completed in 7.14s, cost: $0.0009
2025-10-12 12:08:44,837 - src.methods.rag - INFO - Processing document 26/63 (RAG): TATACONSUM
2025-10-12 12:08:44,838 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q1 2025 (RAG)
2025-10-12 12:08:44,848 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:08:44,857 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:44,857 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:44,857 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:44,881 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:44,881 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:08:51,899 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:51,899 - src.methods.rag - INFO - RAG API call completed in 7.04s, cost: $0.0009
2025-10-12 12:08:51,899 - src.methods.rag - INFO - Processing document 27/63 (RAG): TATACONSUM
2025-10-12 12:08:51,899 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q2 2024 (RAG)
2025-10-12 12:08:51,930 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:08:51,930 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:51,930 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:08:51,930 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:51,964 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:51,964 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:08:57,946 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:08:57,946 - src.methods.rag - INFO - RAG API call completed in 6.02s, cost: $0.0009
2025-10-12 12:08:57,946 - src.methods.rag - INFO - Processing document 28/63 (RAG): TATACONSUM
2025-10-12 12:08:57,946 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q2 2025 (RAG)
2025-10-12 12:08:57,971 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:08:57,971 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:08:57,971 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:08:57,971 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:08:58,002 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:08:58,003 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:09:05,008 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:05,008 - src.methods.rag - INFO - RAG API call completed in 7.04s, cost: $0.0009
2025-10-12 12:09:05,008 - src.methods.rag - INFO - Processing document 29/63 (RAG): TATACONSUM
2025-10-12 12:09:05,008 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q3 2024 (RAG)
2025-10-12 12:09:05,031 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:05,036 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:09:05,036 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:09:05,036 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:09:05,073 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:05,074 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:09:12,071 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:12,071 - src.methods.rag - INFO - RAG API call completed in 7.03s, cost: $0.0009
2025-10-12 12:09:12,071 - src.methods.rag - INFO - Processing document 30/63 (RAG): TATACONSUM
2025-10-12 12:09:12,071 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q3 2025 (RAG)
2025-10-12 12:09:12,094 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:12,097 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:09:12,097 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract all financial metrics...
2025-10-12 12:09:12,098 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:09:12,124 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:12,124 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 6.000000 seconds
2025-10-12 12:09:19,132 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:19,132 - src.methods.rag - INFO - RAG API call completed in 7.03s, cost: $0.0009
2025-10-12 12:09:19,148 - src.methods.rag - INFO - Processing document 31/63 (RAG): TATACONSUM
2025-10-12 12:09:19,148 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q4 2024 (RAG)
2025-10-12 12:09:19,148 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:09:19,164 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:09:19,165 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:09:19,165 - src.methods.rag - WARNING - No contexts retrieved, falling back to empty context
2025-10-12 12:09:19,206 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:19,206 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:09:25,384 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:25,384 - src.methods.rag - INFO - RAG API call completed in 6.22s, cost: $0.0009
2025-10-12 12:09:25,384 - src.methods.rag - INFO - Processing document 32/63 (RAG): TATACONSUM
2025-10-12 12:09:25,384 - src.methods.rag - INFO - Extracting metrics for TATACONSUM Q4 2025 (RAG)
2025-10-12 12:09:25,402 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:25,406 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:09:25,406 - src.methods.rag - INFO - Retrieved 2 contexts in 0.02s for: Extract all financial metrics...
2025-10-12 12:09:25,489 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:25,490 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 9.000000 seconds
2025-10-12 12:09:35,493 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:35,493 - src.methods.rag - INFO - RAG API call completed in 10.09s, cost: $0.0012
2025-10-12 12:09:35,493 - src.methods.rag - INFO - Processing document 33/63 (RAG): BRITANNIA
2025-10-12 12:09:35,493 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q1 2024 (RAG)
2025-10-12 12:09:35,518 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:09:35,522 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:09:35,522 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:09:35,522 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:09:35,555 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:35,556 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:09:40,416 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:40,416 - src.methods.rag - INFO - RAG API call completed in 4.89s, cost: $0.0006
2025-10-12 12:09:40,416 - src.methods.rag - INFO - Processing document 34/63 (RAG): BRITANNIA
2025-10-12 12:09:40,431 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q1 2025 (RAG)
2025-10-12 12:09:40,431 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:09:40,448 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:09:40,448 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:09:40,483 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:40,484 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:09:46,352 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:46,367 - src.methods.rag - INFO - RAG API call completed in 5.92s, cost: $0.0008
2025-10-12 12:09:46,367 - src.methods.rag - INFO - Processing document 35/63 (RAG): BRITANNIA
2025-10-12 12:09:46,367 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q2 2024 (RAG)
2025-10-12 12:09:46,384 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:46,388 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:09:46,388 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:09:46,416 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:46,417 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:09:52,368 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:52,368 - src.methods.rag - INFO - RAG API call completed in 5.98s, cost: $0.0008
2025-10-12 12:09:52,368 - src.methods.rag - INFO - Processing document 36/63 (RAG): BRITANNIA
2025-10-12 12:09:52,368 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q2 2025 (RAG)
2025-10-12 12:09:52,388 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:52,390 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:09:52,390 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:09:52,390 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:09:52,421 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:52,422 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:09:57,164 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:09:57,164 - src.methods.rag - INFO - RAG API call completed in 4.77s, cost: $0.0006
2025-10-12 12:09:57,164 - src.methods.rag - INFO - Processing document 37/63 (RAG): BRITANNIA
2025-10-12 12:09:57,164 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q3 2024 (RAG)
2025-10-12 12:09:57,187 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:09:57,193 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:09:57,194 - src.methods.rag - INFO - Retrieved 1 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:09:57,229 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:09:57,230 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:10:03,196 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:03,196 - src.methods.rag - INFO - RAG API call completed in 6.00s, cost: $0.0008
2025-10-12 12:10:03,196 - src.methods.rag - INFO - Processing document 38/63 (RAG): BRITANNIA
2025-10-12 12:10:03,196 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q3 2025 (RAG)
2025-10-12 12:10:03,214 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:03,219 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:03,220 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:03,220 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:03,248 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:03,249 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:08,056 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:08,056 - src.methods.rag - INFO - RAG API call completed in 4.83s, cost: $0.0006
2025-10-12 12:10:08,071 - src.methods.rag - INFO - Processing document 39/63 (RAG): BRITANNIA
2025-10-12 12:10:08,071 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q4 2024 (RAG)
2025-10-12 12:10:08,071 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:10:08,087 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:10:08,088 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:08,113 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:08,114 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:10:14,204 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:14,204 - src.methods.rag - INFO - RAG API call completed in 6.12s, cost: $0.0008
2025-10-12 12:10:14,204 - src.methods.rag - INFO - Processing document 40/63 (RAG): BRITANNIA
2025-10-12 12:10:14,204 - src.methods.rag - INFO - Extracting commentary for BRITANNIA Q4 2025 (RAG)
2025-10-12 12:10:14,233 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:10:14,235 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:14,235 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:10:14,235 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:14,270 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:14,271 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:18,897 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:18,897 - src.methods.rag - INFO - RAG API call completed in 4.66s, cost: $0.0006
2025-10-12 12:10:18,897 - src.methods.rag - INFO - Processing document 41/63 (RAG): DABUR
2025-10-12 12:10:18,897 - src.methods.rag - INFO - Extracting commentary for DABUR Q1 2024 (RAG)
2025-10-12 12:10:18,912 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:18,912 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:18,912 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:18,912 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:18,943 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:18,943 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:24,056 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:24,071 - src.methods.rag - INFO - RAG API call completed in 5.16s, cost: $0.0006
2025-10-12 12:10:24,075 - src.methods.rag - INFO - Processing document 42/63 (RAG): DABUR
2025-10-12 12:10:24,075 - src.methods.rag - INFO - Extracting commentary for DABUR Q1 2025 (RAG)
2025-10-12 12:10:24,086 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:10:24,093 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:10:24,094 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:24,094 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:24,125 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:24,125 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:28,764 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:28,766 - src.methods.rag - INFO - RAG API call completed in 4.67s, cost: $0.0006
2025-10-12 12:10:28,767 - src.methods.rag - INFO - Processing document 43/63 (RAG): DABUR
2025-10-12 12:10:28,768 - src.methods.rag - INFO - Extracting commentary for DABUR Q2 2024 (RAG)
2025-10-12 12:10:28,782 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:10:28,785 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:28,785 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:28,786 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:28,815 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:28,816 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 3.000000 seconds
2025-10-12 12:10:32,658 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:32,658 - src.methods.rag - INFO - RAG API call completed in 3.87s, cost: $0.0006
2025-10-12 12:10:32,658 - src.methods.rag - INFO - Processing document 44/63 (RAG): DABUR
2025-10-12 12:10:32,658 - src.methods.rag - INFO - Extracting commentary for DABUR Q2 2025 (RAG)
2025-10-12 12:10:32,674 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:10:32,690 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:10:32,691 - src.methods.rag - INFO - Retrieved 1 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:32,708 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:32,708 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:10:38,841 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:38,841 - src.methods.rag - INFO - RAG API call completed in 6.15s, cost: $0.0009
2025-10-12 12:10:38,841 - src.methods.rag - INFO - Processing document 45/63 (RAG): DABUR
2025-10-12 12:10:38,841 - src.methods.rag - INFO - Extracting commentary for DABUR Q3 2024 (RAG)
2025-10-12 12:10:38,862 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:38,865 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:38,865 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:38,866 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:38,894 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:38,895 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2025-10-12 12:10:44,711 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:44,712 - src.methods.rag - INFO - RAG API call completed in 5.85s, cost: $0.0006
2025-10-12 12:10:44,714 - src.methods.rag - INFO - Processing document 46/63 (RAG): DABUR
2025-10-12 12:10:44,714 - src.methods.rag - INFO - Extracting commentary for DABUR Q3 2025 (RAG)
2025-10-12 12:10:44,729 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:44,731 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:44,731 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:44,732 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:44,758 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:44,759 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 3.000000 seconds
2025-10-12 12:10:48,639 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:48,639 - src.methods.rag - INFO - RAG API call completed in 3.91s, cost: $0.0006
2025-10-12 12:10:48,639 - src.methods.rag - INFO - Processing document 47/63 (RAG): DABUR
2025-10-12 12:10:48,639 - src.methods.rag - INFO - Extracting commentary for DABUR Q4 2024 (RAG)
2025-10-12 12:10:48,658 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:48,661 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:48,662 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:10:48,663 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:48,695 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:48,695 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:53,610 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:53,610 - src.methods.rag - INFO - RAG API call completed in 4.95s, cost: $0.0006
2025-10-12 12:10:53,610 - src.methods.rag - INFO - Processing document 48/63 (RAG): DABUR
2025-10-12 12:10:53,610 - src.methods.rag - INFO - Extracting commentary for DABUR Q4 2025 (RAG)
2025-10-12 12:10:53,634 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:53,636 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:10:53,637 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:10:53,637 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:53,660 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:53,694 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:10:58,328 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:10:58,328 - src.methods.rag - INFO - RAG API call completed in 4.69s, cost: $0.0006
2025-10-12 12:10:58,328 - src.methods.rag - INFO - Processing document 49/63 (RAG): MARICO
2025-10-12 12:10:58,328 - src.methods.rag - INFO - Extracting commentary for MARICO Q1 2024 (RAG)
2025-10-12 12:10:58,348 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:10:58,355 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:10:58,355 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:10:58,355 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:10:58,382 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:10:58,388 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:11:03,244 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:11:03,254 - src.methods.rag - INFO - RAG API call completed in 4.90s, cost: $0.0006
2025-10-12 12:11:03,256 - src.methods.rag - INFO - Processing document 50/63 (RAG): MARICO
2025-10-12 12:11:03,257 - src.methods.rag - INFO - Extracting commentary for MARICO Q2 2024 (RAG)
2025-10-12 12:11:03,275 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:03,277 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:03,278 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:11:03,278 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:03,304 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:03,305 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:11:08,054 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:11:08,054 - src.methods.rag - INFO - RAG API call completed in 4.78s, cost: $0.0006
2025-10-12 12:11:08,054 - src.methods.rag - INFO - Processing document 51/63 (RAG): MARICO
2025-10-12 12:11:08,054 - src.methods.rag - INFO - Extracting commentary for MARICO Q2 2025 (RAG)
2025-10-12 12:11:08,083 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:11:08,085 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:08,085 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:11:08,086 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:08,109 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:08,109 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 4.000000 seconds
2025-10-12 12:11:12,933 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:11:12,933 - src.methods.rag - INFO - RAG API call completed in 4.85s, cost: $0.0006
2025-10-12 12:11:12,933 - src.methods.rag - INFO - Processing document 52/63 (RAG): MARICO
2025-10-12 12:11:12,933 - src.methods.rag - INFO - Extracting commentary for MARICO Q3 2024 (RAG)
2025-10-12 12:11:12,955 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:12,958 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:12,959 - src.methods.rag - INFO - Retrieved 1 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:11:12,977 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:12,985 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99381, Requested 804. Please try again in 2m39.785s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:12,987 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99381, Requested 804. Please try again in 2m39.785s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:15,023 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:15,023 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99379, Requested 804. Please try again in 2m37.745s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:15,023 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99379, Requested 804. Please try again in 2m37.745s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:11:19,060 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:19,060 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99374, Requested 804. Please try again in 2m33.71s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:19,060 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-12 12:11:19,060 - src.methods.rag - ERROR - Failed to process document 52: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99374, Requested 804. Please try again in 2m33.71s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:19,060 - src.methods.rag - INFO - Processing document 53/63 (RAG): MARICO
2025-10-12 12:11:19,060 - src.methods.rag - INFO - Extracting commentary for MARICO Q3 2025 (RAG)
2025-10-12 12:11:19,086 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:11:19,089 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:19,090 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:11:19,090 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:19,715 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-12 12:11:19,715 - src.methods.rag - INFO - RAG API call completed in 0.62s, cost: $0.0006
2025-10-12 12:11:19,715 - src.methods.rag - INFO - Processing document 54/63 (RAG): MARICO
2025-10-12 12:11:19,715 - src.methods.rag - INFO - Extracting commentary for MARICO Q4 2024 (RAG)
2025-10-12 12:11:19,736 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:19,739 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:19,740 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:11:19,740 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:19,780 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:19,781 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100266, Requested 593. Please try again in 12m22.975999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:19,781 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100266, Requested 593. Please try again in 12m22.975999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:21,809 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:21,819 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100264, Requested 593. Please try again in 12m20.938999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:21,819 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100264, Requested 593. Please try again in 12m20.938999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:11:25,853 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:25,854 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100259, Requested 593. Please try again in 12m16.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:25,855 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-12 12:11:25,855 - src.methods.rag - ERROR - Failed to process document 54: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100259, Requested 593. Please try again in 12m16.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:25,856 - src.methods.rag - INFO - Processing document 55/63 (RAG): MARICO
2025-10-12 12:11:25,856 - src.methods.rag - INFO - Extracting commentary for MARICO Q4 2025 (RAG)
2025-10-12 12:11:25,871 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:11:25,875 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:25,875 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:11:25,875 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:25,890 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:25,890 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100259, Requested 593. Please try again in 12m16.857s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:25,901 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100259, Requested 593. Please try again in 12m16.857s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:27,941 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:27,942 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100257, Requested 593. Please try again in 12m14.818s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:27,943 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100257, Requested 593. Please try again in 12m14.818s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:11:31,987 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:31,987 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100252, Requested 593. Please try again in 12m10.766s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:31,987 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-12 12:11:31,987 - src.methods.rag - ERROR - Failed to process document 55: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100252, Requested 593. Please try again in 12m10.766s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:31,987 - src.methods.rag - INFO - Processing document 56/63 (RAG): TATACONSUM
2025-10-12 12:11:31,987 - src.methods.rag - INFO - Extracting commentary for TATACONSUM Q1 2024 (RAG)
2025-10-12 12:11:32,011 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:32,013 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:32,014 - src.methods.rag - INFO - Retrieved 0 contexts in 0.03s for: Extract management commentary and key discussion p...
2025-10-12 12:11:32,014 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:32,040 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:32,041 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100252, Requested 599. Please try again in 12m15.9s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:32,041 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100252, Requested 599. Please try again in 12m15.9s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:34,085 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:34,086 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100250, Requested 599. Please try again in 12m13.853999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:34,087 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100250, Requested 599. Please try again in 12m13.853999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:11:38,111 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:38,111 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100245, Requested 599. Please try again in 12m9.818s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:38,111 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-12 12:11:38,127 - src.methods.rag - ERROR - Failed to process document 56: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100245, Requested 599. Please try again in 12m9.818s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:38,127 - src.methods.rag - INFO - Processing document 57/63 (RAG): TATACONSUM
2025-10-12 12:11:38,127 - src.methods.rag - INFO - Extracting commentary for TATACONSUM Q1 2025 (RAG)
2025-10-12 12:11:38,142 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:38,145 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:38,146 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:11:38,146 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:38,174 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:38,175 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100245, Requested 599. Please try again in 12m9.765s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:38,176 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100245, Requested 599. Please try again in 12m9.765s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:40,201 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:40,201 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100243, Requested 599. Please try again in 12m7.732s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:40,201 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100243, Requested 599. Please try again in 12m7.732s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:11:44,260 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:44,260 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100238, Requested 599. Please try again in 12m3.683s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:44,260 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2025-10-12 12:11:44,260 - src.methods.rag - ERROR - Failed to process document 57: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100238, Requested 599. Please try again in 12m3.683s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:44,260 - src.methods.rag - INFO - Processing document 58/63 (RAG): TATACONSUM
2025-10-12 12:11:44,260 - src.methods.rag - INFO - Extracting commentary for TATACONSUM Q2 2024 (RAG)
2025-10-12 12:11:44,281 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:11:44,284 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:11:44,285 - src.methods.rag - INFO - Retrieved 0 contexts in 0.02s for: Extract management commentary and key discussion p...
2025-10-12 12:11:44,285 - src.methods.rag - WARNING - No transcript contexts retrieved
2025-10-12 12:11:44,314 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:44,316 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100238, Requested 599. Please try again in 12m3.626s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:44,316 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100238, Requested 599. Please try again in 12m3.626s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2025-10-12 12:11:46,350 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-10-12 12:11:46,352 - src.methods.rag - ERROR - RAG LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100236, Requested 599. Please try again in 12m1.589s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-10-12 12:11:46,352 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100236, Requested 599. Please try again in 12m1.589s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2025-10-12 12:35:57,304 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-12 12:35:57,304 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-12 12:35:57,304 - __main__ - INFO - ================================================================================
2025-10-12 12:35:57,304 - __main__ - INFO - STEP 5: EVALUATION & COMPARISON
2025-10-12 12:35:57,304 - __main__ - INFO - ================================================================================
2025-10-12 12:35:57,333 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-12 12:35:57,333 - experiment.unified_evaluation - INFO - Started experiment: unified_evaluation
2025-10-12 12:35:57,333 - src.evaluation.evaluator - INFO - Evaluating methods: chain_of_thought, rag, zero_shot
2025-10-12 12:35:57,333 - experiment.unified_evaluation - INFO - Experiment Parameters:
2025-10-12 12:35:57,333 - experiment.unified_evaluation - INFO -   methods: ['chain_of_thought', 'rag', 'zero_shot']
2025-10-12 12:35:57,333 - src.evaluation.evaluator - INFO - Evaluating method: chain_of_thought
2025-10-12 12:35:57,333 - src.evaluation.evaluator - INFO - Evaluating accuracy for chain_of_thought
2025-10-12 12:35:57,713 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2024.json
2025-10-12 12:35:57,728 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2025.json
2025-10-12 12:35:57,745 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2024.json
2025-10-12 12:35:57,749 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2025.json
2025-10-12 12:35:57,765 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2024.json
2025-10-12 12:35:57,772 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2025.json
2025-10-12 12:35:57,782 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q4_2024.json
2025-10-12 12:35:57,782 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for BRITANNIA_Q4_2025.json
2025-10-12 12:35:57,792 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2024.json
2025-10-12 12:35:57,801 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2025.json
2025-10-12 12:35:57,814 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2024.json
2025-10-12 12:35:57,819 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2025.json
2025-10-12 12:35:57,831 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q3_2024.json
2025-10-12 12:35:57,832 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q3_2025.json
2025-10-12 12:35:57,835 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q4_2024.json
2025-10-12 12:35:57,835 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q4_2025.json
2025-10-12 12:35:57,848 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q1_2024.json
2025-10-12 12:35:57,848 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for BRITANNIA_Q4_2025.json
2025-10-12 12:35:57,856 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q3_2025.json
2025-10-12 12:35:57,857 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q4_2025.json
2025-10-12 12:35:57,857 - src.evaluation.evaluator - INFO - Evaluating efficiency for chain_of_thought
2025-10-12 12:35:57,868 - src.utils.helpers - INFO - evaluate_method took 0.53 seconds
2025-10-12 12:35:57,868 - src.evaluation.evaluator - INFO - Evaluating method: rag
2025-10-12 12:35:57,869 - src.evaluation.evaluator - INFO - Evaluating accuracy for rag
2025-10-12 12:35:57,900 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q1_2025.json
2025-10-12 12:35:57,904 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q3_2024.json
2025-10-12 12:35:57,920 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q4_2024.json
2025-10-12 12:35:57,928 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q4_2025.json
2025-10-12 12:35:57,939 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q1_2024.json
2025-10-12 12:35:57,951 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q1_2025.json
2025-10-12 12:35:57,955 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q2_2024.json
2025-10-12 12:35:57,966 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q2_2025.json
2025-10-12 12:35:57,976 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q3_2024.json
2025-10-12 12:35:57,985 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q3_2025.json
2025-10-12 12:35:57,996 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q4_2024.json
2025-10-12 12:35:58,003 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q4_2025.json
2025-10-12 12:35:58,364 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2024.json
2025-10-12 12:35:58,373 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2025.json
2025-10-12 12:35:58,382 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2024.json
2025-10-12 12:35:58,387 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2025.json
2025-10-12 12:35:58,400 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2024.json
2025-10-12 12:35:58,403 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2025.json
2025-10-12 12:35:58,420 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q4_2024.json
2025-10-12 12:35:58,428 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q4_2025.json
2025-10-12 12:35:58,436 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2024.json
2025-10-12 12:35:58,447 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2025.json
2025-10-12 12:35:58,453 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2024.json
2025-10-12 12:35:58,466 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2025.json
2025-10-12 12:35:58,472 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q3_2024.json
2025-10-12 12:35:58,482 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q3_2025.json
2025-10-12 12:35:58,493 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q4_2024.json
2025-10-12 12:35:58,505 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q4_2025.json
2025-10-12 12:35:58,513 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q1_2024.json
2025-10-12 12:35:58,521 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q2_2024.json
2025-10-12 12:35:58,533 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q2_2025.json
2025-10-12 12:35:58,538 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q3_2025.json
2025-10-12 12:35:58,546 - src.evaluation.evaluator - INFO - Evaluating efficiency for rag
2025-10-12 12:35:58,549 - src.utils.helpers - INFO - evaluate_method took 0.68 seconds
2025-10-12 12:35:58,549 - src.evaluation.evaluator - INFO - Evaluating method: zero_shot
2025-10-12 12:35:58,549 - src.evaluation.evaluator - INFO - Evaluating accuracy for zero_shot
2025-10-12 12:35:58,565 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2024.json
2025-10-12 12:35:58,578 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q1_2025.json
2025-10-12 12:35:58,586 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2024.json
2025-10-12 12:35:58,596 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q2_2025.json
2025-10-12 12:35:58,606 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2024.json
2025-10-12 12:35:58,616 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q3_2025.json
2025-10-12 12:35:58,620 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q4_2024.json
2025-10-12 12:35:58,634 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for BRITANNIA_Q4_2025.json
2025-10-12 12:35:58,638 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2024.json
2025-10-12 12:35:58,649 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q1_2025.json
2025-10-12 12:35:58,658 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2024.json
2025-10-12 12:35:58,665 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q2_2025.json
2025-10-12 12:35:58,676 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q3_2024.json
2025-10-12 12:35:58,687 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q3_2025.json
2025-10-12 12:35:58,696 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q4_2024.json
2025-10-12 12:35:58,718 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for DABUR_Q4_2025.json
2025-10-12 12:35:58,731 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q1_2024.json
2025-10-12 12:35:58,756 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q1_2025.json
2025-10-12 12:35:58,768 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q2_2024.json
2025-10-12 12:35:58,780 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q2_2025.json
2025-10-12 12:35:58,780 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q3_2024.json
2025-10-12 12:35:58,802 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q4_2025.json
2025-10-12 12:35:58,815 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q1_2024.json
2025-10-12 12:35:58,821 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q1_2025.json
2025-10-12 12:35:58,833 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q2_2024.json
2025-10-12 12:35:58,851 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q2_2025.json
2025-10-12 12:35:58,852 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q3_2024.json
2025-10-12 12:35:58,873 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q3_2025.json
2025-10-12 12:35:58,884 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q4_2024.json
2025-10-12 12:35:58,887 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for TATACONSUM_Q4_2025.json
2025-10-12 12:35:58,899 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q3_2025.json
2025-10-12 12:35:58,916 - src.evaluation.accuracy_metrics - WARNING - No ground truth found for MARICO_Q4_2024.json
2025-10-12 12:35:58,921 - src.evaluation.evaluator - INFO - Evaluating efficiency for zero_shot
2025-10-12 12:35:58,934 - src.utils.helpers - INFO - evaluate_method took 0.38 seconds
2025-10-12 12:35:58,935 - experiment.unified_evaluation - INFO - Metrics:
2025-10-12 12:35:58,936 - experiment.unified_evaluation - INFO -   methods_evaluated: 3
2025-10-12 12:35:58,936 - experiment.unified_evaluation - INFO -   best_accuracy_method: None
2025-10-12 12:35:58,936 - experiment.unified_evaluation - INFO -   best_efficiency_method: rag
2025-10-12 12:35:58,937 - src.utils.helpers - INFO - evaluate_all_methods took 1.60 seconds
2025-10-12 12:35:58,951 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251012_123558.json
2025-10-12 12:35:58,965 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251012_123558.csv
2025-10-12 12:35:58,965 - src.utils.helpers - INFO - run_evaluation took 1.66 seconds
2025-10-12 12:35:58,966 - __main__ - ERROR - 
Pipeline failed with error: 'accuracy'
Traceback (most recent call last):
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 721, in main
    pipeline.run_evaluation()
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\utils\helpers.py", line 45, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\main.py", line 493, in run_evaluation
    html_report = self.evaluator.generate_evaluation_report(
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\evaluation\evaluator.py", line 416, in generate_evaluation_report
    html_content = self._generate_html_report(comparison_results)
  File "C:\Users\lenovo\Research-Finance\Financial-Metrics-Extraction-with-LLMs\src\evaluation\evaluator.py", line 488, in _generate_html_report
    methods = accuracy['financial_metrics']['accuracy']['values'].keys()
KeyError: 'accuracy'
2025-10-12 12:38:23,511 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-12 12:38:27,477 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-12 12:38:27,477 - src.utils.helpers - INFO - _initialize_model took 3.97 seconds
2025-10-12 12:38:27,486 - src.data_preprocessing.embed_store - INFO - Loaded FAISS index from data\embeddings\financial_index.faiss
2025-10-12 12:38:28,951 - src.utils.helpers - INFO - generate_embeddings took 1.46 seconds
2025-10-12 12:38:28,966 - src.utils.helpers - INFO - search took 0.02 seconds
2025-10-12 12:38:28,993 - src.utils.helpers - INFO - generate_embeddings took 0.03 seconds
2025-10-12 12:38:29,006 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 12:38:29,093 - src.utils.helpers - INFO - generate_embeddings took 0.09 seconds
2025-10-12 12:38:29,093 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:55:26,862 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-12 12:55:30,771 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-12 12:55:30,771 - src.utils.helpers - INFO - _initialize_model took 3.91 seconds
2025-10-12 12:55:30,779 - src.data_preprocessing.embed_store - INFO - Loaded FAISS index from data\embeddings\financial_index.faiss
2025-10-12 12:55:30,795 - src.utils.resume_extraction - INFO - RAG Resume Stats: 32/63 completed
2025-10-12 12:55:30,800 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q1_2024
2025-10-12 12:55:30,800 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q1_2025
2025-10-12 12:55:30,800 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q2_2024
2025-10-12 12:55:30,807 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q2_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q3_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q3_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q4_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q4_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q1_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q1_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q2_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q2_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q3_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q3_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q4_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q4_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q1_2024
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q1_2025
2025-10-12 12:55:30,808 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q2_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q2_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q3_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q3_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q4_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q4_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q1_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q1_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q2_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q2_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q3_2024
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q3_2025
2025-10-12 12:55:30,814 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q4_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q4_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q1_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q1_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q2_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q2_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q3_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q3_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q4_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: BRITANNIA_Q4_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q1_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q1_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q2_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q2_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q3_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q3_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q4_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: DABUR_Q4_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q1_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q2_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q2_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q3_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q3_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q4_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: MARICO_Q4_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q1_2024
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q1_2025
2025-10-12 12:55:30,818 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q2_2024
2025-10-12 12:55:30,826 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q2_2025
2025-10-12 12:55:30,826 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q3_2024
2025-10-12 12:55:30,826 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q3_2025
2025-10-12 12:55:30,826 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q4_2024
2025-10-12 12:55:30,828 - src.utils.resume_extraction - INFO - Skipping already processed: TATACONSUM_Q4_2025
2025-10-12 12:55:30,828 - src.utils.resume_extraction - INFO - Found 32 processed documents, 0 remaining to process
2025-10-12 12:55:30,828 - src.utils.resume_extraction - INFO -  All documents already processed for RAG!
2025-10-12 12:57:38,879 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-10-12 12:57:42,750 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2025-10-12 12:57:42,750 - src.utils.helpers - INFO - _initialize_model took 3.87 seconds
2025-10-12 12:57:42,779 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1071 chunks from financial_chunks_fixed_size.json
2025-10-12 12:58:33,078 - src.utils.helpers - INFO - generate_embeddings took 50.30 seconds
2025-10-12 12:58:33,098 - src.utils.helpers - INFO - embed_chunks_from_file took 50.35 seconds
2025-10-12 12:58:33,098 - src.data_preprocessing.embed_store - INFO - Processed 1071 financial chunks
2025-10-12 12:58:33,132 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 1623 chunks from transcript_chunks_fixed_size.json
2025-10-12 12:59:43,478 - src.utils.helpers - INFO - generate_embeddings took 70.35 seconds
2025-10-12 12:59:43,529 - src.utils.helpers - INFO - embed_chunks_from_file took 70.43 seconds
2025-10-12 12:59:43,529 - src.data_preprocessing.embed_store - INFO - Processed 1623 transcript chunks
2025-10-12 12:59:43,699 - src.data_preprocessing.embed_store - INFO - Added 2694 embeddings to FAISS index
2025-10-12 12:59:43,699 - src.utils.helpers - INFO - add_embeddings took 0.12 seconds
2025-10-12 12:59:43,715 - src.data_preprocessing.embed_store - INFO - Saved FAISS index to data\embeddings\financial_index.faiss
2025-10-12 12:59:43,796 - src.data_preprocessing.embed_store - INFO - Created vector store with 2694 chunks
2025-10-12 12:59:43,814 - src.utils.helpers - INFO - create_vector_store_from_chunks took 121.06 seconds
2025-10-12 12:59:43,829 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:59:43,832 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:59:43,847 - src.utils.helpers - INFO - generate_embeddings took 0.02 seconds
2025-10-12 12:59:43,848 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:59:43,860 - src.utils.helpers - INFO - generate_embeddings took 0.01 seconds
2025-10-12 12:59:43,864 - src.utils.helpers - INFO - search took 0.00 seconds
2025-10-12 12:59:43,864 - src.utils.helpers - INFO - generate_embeddings took 0.00 seconds
2025-10-12 12:59:43,879 - src.utils.helpers - INFO - search took 0.01 seconds
2025-10-12 13:25:06,418 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2025-10-12 13:25:06,418 - __main__ - INFO - Initialized Financial RAG Pipeline
2025-10-12 13:25:06,418 - __main__ - INFO - ================================================================================
2025-10-12 13:25:06,418 - __main__ - INFO - STEP 5: EVALUATION & COMPARISON
2025-10-12 13:25:06,418 - __main__ - INFO - ================================================================================
2025-10-12 13:25:06,445 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2025-10-12 13:25:06,451 - experiment.unified_evaluation - INFO - Started experiment: unified_evaluation
2025-10-12 13:25:06,451 - src.evaluation.evaluator - INFO - Evaluating methods: chain_of_thought, rag, zero_shot
2025-10-12 13:25:06,451 - experiment.unified_evaluation - INFO - Experiment Parameters:
2025-10-12 13:25:06,451 - experiment.unified_evaluation - INFO -   methods: ['chain_of_thought', 'rag', 'zero_shot']
2025-10-12 13:25:06,451 - src.evaluation.evaluator - INFO - Evaluating method: chain_of_thought
2025-10-12 13:25:06,451 - src.evaluation.evaluator - INFO - Evaluating accuracy for chain_of_thought
2025-10-12 13:25:06,475 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for BRITANNIA_Q4_2025.json
2025-10-12 13:25:06,485 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q3_2025.json
2025-10-12 13:25:06,485 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q4_2025.json
2025-10-12 13:25:06,490 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for BRITANNIA_Q4_2025.json
2025-10-12 13:25:06,495 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q3_2025.json
2025-10-12 13:25:06,495 - src.evaluation.accuracy_metrics - WARNING - Cannot determine document type for DABUR_Q4_2025.json
2025-10-12 13:25:06,495 - src.evaluation.evaluator - INFO - Evaluating efficiency for chain_of_thought
2025-10-12 13:25:06,503 - src.utils.helpers - INFO - evaluate_method took 0.05 seconds
2025-10-12 13:25:06,503 - src.evaluation.evaluator - INFO - Evaluating method: rag
2025-10-12 13:25:06,503 - src.evaluation.evaluator - INFO - Evaluating accuracy for rag
2025-10-12 13:25:06,588 - src.evaluation.evaluator - INFO - Evaluating efficiency for rag
2025-10-12 13:25:06,606 - src.utils.helpers - INFO - evaluate_method took 0.10 seconds
2025-10-12 13:25:06,606 - src.evaluation.evaluator - INFO - Evaluating method: zero_shot
2025-10-12 13:25:06,606 - src.evaluation.evaluator - INFO - Evaluating accuracy for zero_shot
2025-10-12 13:25:06,676 - src.evaluation.evaluator - INFO - Evaluating efficiency for zero_shot
2025-10-12 13:25:06,693 - src.utils.helpers - INFO - evaluate_method took 0.09 seconds
2025-10-12 13:25:06,693 - experiment.unified_evaluation - INFO - Metrics:
2025-10-12 13:25:06,693 - experiment.unified_evaluation - INFO -   methods_evaluated: 3
2025-10-12 13:25:06,693 - experiment.unified_evaluation - INFO -   best_accuracy_method: rag
2025-10-12 13:25:06,693 - experiment.unified_evaluation - INFO -   best_efficiency_method: rag
2025-10-12 13:25:06,693 - src.utils.helpers - INFO - evaluate_all_methods took 0.24 seconds
2025-10-12 13:25:06,727 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251012_132506.json
2025-10-12 13:25:06,728 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251012_132506.csv
2025-10-12 13:25:06,736 - src.evaluation.evaluator - INFO - Generated evaluation report: results\evaluation_reports\evaluation_report_20251012_132506.html
2025-10-12 13:25:06,736 - __main__ - INFO - 
Evaluation reports generated:
2025-10-12 13:25:06,736 - __main__ - INFO -   - JSON: results\evaluation_reports\evaluation_report_20251012_132506.json
2025-10-12 13:25:06,736 - __main__ - INFO -   - CSV: results\evaluation_reports\evaluation_report_20251012_132506.csv
2025-10-12 13:25:06,736 - __main__ - INFO -   - HTML: results\evaluation_reports\evaluation_report_20251012_132506.html
2025-10-12 13:25:06,736 - src.utils.helpers - INFO - run_evaluation took 0.32 seconds
2026-01-28 18:36:31,350 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2026-01-28 18:36:31,380 - __main__ - INFO - Initialized Financial RAG Pipeline
2026-01-28 18:36:31,380 - __main__ - INFO - ================================================================================
2026-01-28 18:36:31,380 - __main__ - INFO - STEP 1: DATA PREPROCESSING
2026-01-28 18:36:31,380 - __main__ - INFO - ================================================================================
2026-01-28 18:36:31,402 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2026-01-28 18:45:58,337 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2026-01-28 18:45:58,370 - __main__ - INFO - Initialized Financial RAG Pipeline
2026-01-28 18:45:58,371 - __main__ - INFO - ================================================================================
2026-01-28 18:45:58,394 - __main__ - INFO - STEP 2: DOCUMENT CHUNKING
2026-01-28 18:45:58,394 - __main__ - INFO - ================================================================================
2026-01-28 18:45:58,428 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2026-01-28 18:45:58,584 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:58,600 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2024.json: 37 chunks
2026-01-28 18:45:58,737 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:58,737 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2025.json: 28 chunks
2026-01-28 18:45:58,772 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:58,772 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2024.json: 49 chunks
2026-01-28 18:45:58,984 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:58,984 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2025.json: 43 chunks
2026-01-28 18:45:59,022 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2026-01-28 18:45:59,045 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2024.json: 42 chunks
2026-01-28 18:45:59,122 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2026-01-28 18:45:59,122 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2025.json: 31 chunks
2026-01-28 18:45:59,189 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,189 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2024.json: 69 chunks
2026-01-28 18:45:59,276 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.02 seconds
2026-01-28 18:45:59,276 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2025.json: 56 chunks
2026-01-28 18:45:59,379 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,380 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2024.json: 31 chunks
2026-01-28 18:45:59,423 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2026-01-28 18:45:59,423 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2025.json: 31 chunks
2026-01-28 18:45:59,554 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,554 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2024.json: 43 chunks
2026-01-28 18:45:59,595 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,595 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2025.json: 44 chunks
2026-01-28 18:45:59,645 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,645 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2024.json: 34 chunks
2026-01-28 18:45:59,695 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,695 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2025.json: 35 chunks
2026-01-28 18:45:59,728 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,728 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2024.json: 29 chunks
2026-01-28 18:45:59,745 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2026-01-28 18:45:59,745 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2025.json: 55 chunks
2026-01-28 18:45:59,789 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,789 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2024.json: 46 chunks
2026-01-28 18:45:59,811 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,812 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q1_2025.json: 50 chunks
2026-01-28 18:45:59,845 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,845 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2024.json: 47 chunks
2026-01-28 18:45:59,879 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,879 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q2_2025.json: 46 chunks
2026-01-28 18:45:59,912 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,912 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2024.json: 50 chunks
2026-01-28 18:45:59,929 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,929 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q3_2025.json: 46 chunks
2026-01-28 18:45:59,962 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:45:59,962 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2024.json: 46 chunks
2026-01-28 18:46:00,183 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,183 - src.data_preprocessing.chunking - INFO - Chunked BRITANNIA_Q4_2025.json: 48 chunks
2026-01-28 18:46:00,229 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,229 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2024.json: 62 chunks
2026-01-28 18:46:00,246 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,246 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q1_2025.json: 59 chunks
2026-01-28 18:46:00,372 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,372 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2024.json: 38 chunks
2026-01-28 18:46:00,392 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,392 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q2_2025.json: 69 chunks
2026-01-28 18:46:00,412 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,412 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2024.json: 52 chunks
2026-01-28 18:46:00,427 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,427 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q3_2025.json: 63 chunks
2026-01-28 18:46:00,454 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.01 seconds
2026-01-28 18:46:00,454 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2024.json: 75 chunks
2026-01-28 18:46:00,456 - src.utils.helpers - INFO - chunk_by_fixed_size took 0.00 seconds
2026-01-28 18:46:00,456 - src.data_preprocessing.chunking - INFO - Chunked DABUR_Q4_2025.json: 47 chunks
2026-01-28 18:46:00,456 - src.utils.helpers - INFO - chunk_all_documents took 2.03 seconds
2026-01-28 18:46:00,498 - src.data_preprocessing.chunking - INFO - Saved 657 financial chunks to financial_chunks_fixed_size.json
2026-01-28 18:46:00,522 - src.data_preprocessing.chunking - INFO - Saved 844 transcript chunks to transcript_chunks_fixed_size.json
2026-01-28 18:46:00,562 - experiment.financial_rag_pipeline - INFO - Metrics:
2026-01-28 18:46:00,562 - experiment.financial_rag_pipeline - INFO -   chunking_strategy: fixed_size
2026-01-28 18:46:00,562 - experiment.financial_rag_pipeline - INFO -   chunking_total_chunks: 1501
2026-01-28 18:46:00,562 - experiment.financial_rag_pipeline - INFO -   chunking_financial: 657
2026-01-28 18:46:00,562 - experiment.financial_rag_pipeline - INFO -   chunking_transcript: 844
2026-01-28 18:46:00,562 - src.utils.helpers - INFO - run_chunking took 2.19 seconds
2026-01-28 18:56:10,356 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2026-01-28 18:56:10,356 - __main__ - INFO - Initialized Financial RAG Pipeline
2026-01-28 18:56:10,356 - __main__ - INFO - ================================================================================
2026-01-28 18:56:10,356 - __main__ - INFO - STEP 3: EMBEDDING GENERATION & VECTOR STORE
2026-01-28 18:56:10,356 - __main__ - INFO - ================================================================================
2026-01-28 18:56:10,380 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2026-01-28 18:56:10,413 - src.utils.config_loader - INFO - Loaded configuration from config.yaml
2026-01-28 18:56:10,416 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-28 18:56:15,264 - src.data_preprocessing.embed_store - INFO - Loaded embedding model: all-MiniLM-L6-v2
2026-01-28 18:56:15,264 - src.utils.helpers - INFO - _initialize_model took 4.85 seconds
2026-01-28 18:56:15,297 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 657 chunks from financial_chunks_fixed_size.json
2026-01-28 18:56:53,535 - src.utils.helpers - INFO - generate_embeddings took 38.24 seconds
2026-01-28 18:56:53,550 - src.utils.helpers - INFO - embed_chunks_from_file took 38.29 seconds
2026-01-28 18:56:53,550 - src.data_preprocessing.embed_store - INFO - Processed 657 financial chunks
2026-01-28 18:56:53,771 - src.data_preprocessing.embed_store - INFO - Generating embeddings for 844 chunks from transcript_chunks_fixed_size.json
2026-01-28 18:57:36,967 - src.utils.helpers - INFO - generate_embeddings took 43.20 seconds
2026-01-28 18:57:36,989 - src.utils.helpers - INFO - embed_chunks_from_file took 43.44 seconds
2026-01-28 18:57:36,999 - src.data_preprocessing.embed_store - INFO - Processed 844 transcript chunks
2026-01-28 18:57:37,434 - src.data_preprocessing.embed_store - INFO - Added 1501 embeddings to FAISS index
2026-01-28 18:57:37,435 - src.utils.helpers - INFO - add_embeddings took 0.41 seconds
2026-01-28 18:57:37,437 - src.data_preprocessing.embed_store - INFO - Saved FAISS index to data\embeddings\financial_index.faiss
2026-01-28 18:57:37,453 - src.data_preprocessing.embed_store - INFO - Created vector store with 1501 chunks
2026-01-28 18:57:37,468 - src.utils.helpers - INFO - create_vector_store_from_chunks took 82.20 seconds
2026-01-28 18:57:37,468 - experiment.financial_rag_pipeline - INFO - Metrics:
2026-01-28 18:57:37,468 - experiment.financial_rag_pipeline - INFO -   embedding_model: all-MiniLM-L6-v2
2026-01-28 18:57:37,468 - experiment.financial_rag_pipeline - INFO -   embedding_dimension: 384
2026-01-28 18:57:37,468 - experiment.financial_rag_pipeline - INFO -   embedding_num_chunks: 1501
2026-01-28 18:57:37,468 - src.utils.helpers - INFO - run_embedding_generation took 87.11 seconds
2026-01-28 19:06:43,753 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2026-01-28 19:06:43,787 - __main__ - INFO - Initialized Financial RAG Pipeline
2026-01-28 19:06:43,787 - __main__ - WARNING - GROQ_API_KEY not found in environment
2026-01-28 19:06:44,698 - __main__ - INFO - Loaded 32 documents for extraction
2026-01-28 19:06:44,703 - __main__ - INFO - ================================================================================
2026-01-28 19:06:44,713 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2026-01-28 19:06:44,714 - __main__ - INFO - ================================================================================
2026-01-28 19:06:44,714 - __main__ - ERROR - Groq API client not available for model: llama-3.3-70b-versatile
2026-01-28 19:06:44,715 - src.utils.helpers - INFO - run_zero_shot_extraction took 0.01 seconds
2026-01-28 19:49:26,960 - experiment.financial_rag_pipeline - INFO - Started experiment: financial_rag_pipeline
2026-01-28 19:49:27,224 - __main__ - INFO - Initialized Financial RAG Pipeline
2026-01-28 19:49:27,584 - __main__ - INFO - Groq client initialized
2026-01-28 19:49:45,574 - __main__ - INFO - Loaded 32 documents for extraction
2026-01-28 19:49:45,575 - __main__ - INFO - ================================================================================
2026-01-28 19:49:45,575 - __main__ - INFO - STEP 4a: ZERO-SHOT EXTRACTION
2026-01-28 19:49:45,576 - __main__ - INFO - ================================================================================
2026-01-28 19:49:45,878 - src.methods.zero_shot - INFO - Starting batch extraction for 32 documents
2026-01-28 19:49:45,878 - src.methods.zero_shot - INFO - Processing document 1/32: BRITANNIA (financial)
2026-01-28 19:49:45,894 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2024 (Zero-shot)
2026-01-28 19:49:45,894 - src.methods.zero_shot - WARNING - Truncating text from 36706 to 20000 chars
2026-01-28 19:49:53,881 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:49:53,901 - src.methods.zero_shot - INFO - API call completed in 8.01s, cost: $0.0041
2026-01-28 19:49:53,901 - src.utils.helpers - INFO - LLM call for BRITANNIA took 8.01 seconds
2026-01-28 19:49:53,901 - src.methods.zero_shot - INFO - Processing document 2/32: BRITANNIA (financial)
2026-01-28 19:49:53,901 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q1 2025 (Zero-shot)
2026-01-28 19:49:53,901 - src.methods.zero_shot - WARNING - Truncating text from 27637 to 20000 chars
2026-01-28 19:49:53,965 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:49:53,965 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds
2026-01-28 19:50:00,210 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:50:00,210 - src.methods.zero_shot - INFO - API call completed in 6.31s, cost: $0.0039
2026-01-28 19:50:00,210 - src.utils.helpers - INFO - LLM call for BRITANNIA took 6.31 seconds
2026-01-28 19:50:00,210 - src.methods.zero_shot - INFO - Processing document 3/32: BRITANNIA (financial)
2026-01-28 19:50:00,210 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2024 (Zero-shot)
2026-01-28 19:50:00,210 - src.methods.zero_shot - WARNING - Truncating text from 48338 to 20000 chars
2026-01-28 19:50:00,442 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:50:00,442 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2026-01-28 19:50:36,327 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:50:36,343 - src.methods.zero_shot - INFO - API call completed in 36.13s, cost: $0.0042
2026-01-28 19:50:36,343 - src.utils.helpers - INFO - LLM call for BRITANNIA took 36.13 seconds
2026-01-28 19:50:36,360 - src.methods.zero_shot - INFO - Processing document 4/32: BRITANNIA (financial)
2026-01-28 19:50:36,361 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q2 2025 (Zero-shot)
2026-01-28 19:50:36,361 - src.methods.zero_shot - WARNING - Truncating text from 41769 to 20000 chars
2026-01-28 19:50:36,378 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:50:36,378 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2026-01-28 19:51:09,735 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:51:09,735 - src.methods.zero_shot - INFO - API call completed in 33.37s, cost: $0.0040
2026-01-28 19:51:09,751 - src.utils.helpers - INFO - LLM call for BRITANNIA took 33.39 seconds
2026-01-28 19:51:09,751 - src.methods.zero_shot - INFO - Processing document 5/32: BRITANNIA (financial)
2026-01-28 19:51:09,751 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2024 (Zero-shot)
2026-01-28 19:51:09,751 - src.methods.zero_shot - WARNING - Truncating text from 40740 to 20000 chars
2026-01-28 19:51:09,805 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:51:09,805 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2026-01-28 19:51:43,126 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:51:43,129 - src.methods.zero_shot - INFO - API call completed in 33.38s, cost: $0.0041
2026-01-28 19:51:43,129 - src.utils.helpers - INFO - LLM call for BRITANNIA took 33.38 seconds
2026-01-28 19:51:43,131 - src.methods.zero_shot - INFO - Processing document 6/32: BRITANNIA (financial)
2026-01-28 19:51:43,131 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q3 2025 (Zero-shot)
2026-01-28 19:51:43,132 - src.methods.zero_shot - WARNING - Truncating text from 30274 to 20000 chars
2026-01-28 19:51:43,159 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:51:43,159 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2026-01-28 19:52:18,655 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:52:18,657 - src.methods.zero_shot - INFO - API call completed in 35.52s, cost: $0.0042
2026-01-28 19:52:18,657 - src.utils.helpers - INFO - LLM call for BRITANNIA took 35.52 seconds
2026-01-28 19:52:18,660 - src.methods.zero_shot - INFO - Processing document 7/32: BRITANNIA (financial)
2026-01-28 19:52:18,660 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2024 (Zero-shot)
2026-01-28 19:52:18,661 - src.methods.zero_shot - WARNING - Truncating text from 67837 to 20000 chars
2026-01-28 19:52:18,689 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:52:18,697 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2026-01-28 19:52:53,871 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:52:53,875 - src.methods.zero_shot - INFO - API call completed in 35.21s, cost: $0.0042
2026-01-28 19:52:53,875 - src.utils.helpers - INFO - LLM call for BRITANNIA took 35.21 seconds
2026-01-28 19:52:53,949 - src.methods.zero_shot - INFO - Processing document 8/32: BRITANNIA (financial)
2026-01-28 19:52:53,949 - src.methods.zero_shot - INFO - Extracting metrics for BRITANNIA Q4 2025 (Zero-shot)
2026-01-28 19:52:53,949 - src.methods.zero_shot - WARNING - Truncating text from 54957 to 20000 chars
2026-01-28 19:52:53,988 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:52:53,988 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 31.000000 seconds
2026-01-28 19:53:26,190 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:53:26,192 - src.methods.zero_shot - INFO - API call completed in 32.24s, cost: $0.0039
2026-01-28 19:53:26,192 - src.utils.helpers - INFO - LLM call for BRITANNIA took 32.24 seconds
2026-01-28 19:53:26,194 - src.methods.zero_shot - INFO - Processing document 9/32: DABUR (financial)
2026-01-28 19:53:26,194 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2024 (Zero-shot)
2026-01-28 19:53:26,195 - src.methods.zero_shot - WARNING - Truncating text from 30096 to 20000 chars
2026-01-28 19:53:26,224 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:53:26,224 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2026-01-28 19:53:59,452 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:53:59,454 - src.methods.zero_shot - INFO - API call completed in 33.26s, cost: $0.0039
2026-01-28 19:53:59,454 - src.utils.helpers - INFO - LLM call for DABUR took 33.26 seconds
2026-01-28 19:53:59,456 - src.methods.zero_shot - INFO - Processing document 10/32: DABUR (financial)
2026-01-28 19:53:59,456 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q1 2025 (Zero-shot)
2026-01-28 19:53:59,457 - src.methods.zero_shot - WARNING - Truncating text from 30565 to 20000 chars
2026-01-28 19:53:59,504 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:53:59,505 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 31.000000 seconds
2026-01-28 19:54:31,696 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:54:31,697 - src.methods.zero_shot - INFO - API call completed in 32.24s, cost: $0.0039
2026-01-28 19:54:31,698 - src.utils.helpers - INFO - LLM call for DABUR took 32.24 seconds
2026-01-28 19:54:31,700 - src.methods.zero_shot - INFO - Processing document 11/32: DABUR (financial)
2026-01-28 19:54:31,700 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q2 2024 (Zero-shot)
2026-01-28 19:54:31,701 - src.methods.zero_shot - WARNING - Truncating text from 42616 to 20000 chars
2026-01-28 19:54:31,732 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:54:31,733 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 35.000000 seconds
2026-01-28 19:55:08,068 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:55:08,070 - src.methods.zero_shot - INFO - API call completed in 36.37s, cost: $0.0043
2026-01-28 19:55:08,070 - src.utils.helpers - INFO - LLM call for DABUR took 36.37 seconds
2026-01-28 19:55:08,086 - src.methods.zero_shot - INFO - Processing document 12/32: DABUR (financial)
2026-01-28 19:55:08,086 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q2 2025 (Zero-shot)
2026-01-28 19:55:08,087 - src.methods.zero_shot - WARNING - Truncating text from 43845 to 20000 chars
2026-01-28 19:55:08,126 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:55:08,127 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 36.000000 seconds
2026-01-28 19:55:45,186 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:55:45,272 - src.methods.zero_shot - INFO - API call completed in 37.18s, cost: $0.0045
2026-01-28 19:55:45,272 - src.utils.helpers - INFO - LLM call for DABUR took 37.18 seconds
2026-01-28 19:55:45,317 - src.methods.zero_shot - INFO - Processing document 13/32: DABUR (financial)
2026-01-28 19:55:45,317 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q3 2024 (Zero-shot)
2026-01-28 19:55:45,318 - src.methods.zero_shot - WARNING - Truncating text from 33178 to 20000 chars
2026-01-28 19:55:45,354 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:55:45,356 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 34.000000 seconds
2026-01-28 19:56:21,266 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:56:21,268 - src.methods.zero_shot - INFO - API call completed in 35.95s, cost: $0.0042
2026-01-28 19:56:21,268 - src.utils.helpers - INFO - LLM call for DABUR took 35.95 seconds
2026-01-28 19:56:21,289 - src.methods.zero_shot - INFO - Processing document 14/32: DABUR (financial)
2026-01-28 19:56:21,290 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q3 2025 (Zero-shot)
2026-01-28 19:56:21,290 - src.methods.zero_shot - WARNING - Truncating text from 34607 to 20000 chars
2026-01-28 19:56:21,362 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:56:21,363 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 32.000000 seconds
2026-01-28 19:56:54,522 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2026-01-28 19:56:54,538 - src.methods.zero_shot - INFO - API call completed in 33.25s, cost: $0.0041
2026-01-28 19:56:54,538 - src.utils.helpers - INFO - LLM call for DABUR took 33.25 seconds
2026-01-28 19:56:54,562 - src.methods.zero_shot - INFO - Processing document 15/32: DABUR (financial)
2026-01-28 19:56:54,562 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q4 2024 (Zero-shot)
2026-01-28 19:56:54,563 - src.methods.zero_shot - WARNING - Truncating text from 28653 to 20000 chars
2026-01-28 19:56:54,596 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:56:54,652 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95940, Requested 8292. Please try again in 1h0m56.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:56:54,653 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95940, Requested 8292. Please try again in 1h0m56.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:56:56,717 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:56:56,718 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95938, Requested 8292. Please try again in 1h0m54.72s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:56:56,719 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95938, Requested 8292. Please try again in 1h0m54.72s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:00,794 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:00,795 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 8292. Please try again in 1h0m50.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:00,796 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:00,796 - src.utils.helpers - INFO - LLM call for DABUR took 6.23 seconds
2026-01-28 19:57:00,797 - src.methods.zero_shot - ERROR - Failed to process document 15: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 8292. Please try again in 1h0m50.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:00,798 - src.methods.zero_shot - INFO - Processing document 16/32: DABUR (financial)
2026-01-28 19:57:00,798 - src.methods.zero_shot - INFO - Extracting metrics for DABUR Q4 2025 (Zero-shot)
2026-01-28 19:57:00,798 - src.methods.zero_shot - WARNING - Truncating text from 54253 to 20000 chars
2026-01-28 19:57:00,836 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:00,837 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5737. Please try again in 24m2.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:00,838 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95933, Requested 5737. Please try again in 24m2.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:02,876 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:02,877 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95931, Requested 5737. Please try again in 24m1.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:02,878 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95931, Requested 5737. Please try again in 24m1.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:06,929 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:06,930 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 5737. Please try again in 23m56.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:06,931 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:06,931 - src.utils.helpers - INFO - LLM call for DABUR took 6.13 seconds
2026-01-28 19:57:06,932 - src.methods.zero_shot - ERROR - Failed to process document 16: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 5737. Please try again in 23m56.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:06,933 - src.methods.zero_shot - INFO - Processing document 17/32: BRITANNIA (transcript)
2026-01-28 19:57:06,933 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q1 2024 (Zero-shot)
2026-01-28 19:57:06,933 - src.methods.zero_shot - WARNING - Truncating text from 45109 to 20000 chars
2026-01-28 19:57:06,971 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:06,972 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 4927. Please try again in 12m16.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:06,972 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95926, Requested 4927. Please try again in 12m16.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:09,004 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:09,006 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95924, Requested 4927. Please try again in 12m15.264s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:09,006 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95924, Requested 4927. Please try again in 12m15.264s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:13,068 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:13,068 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4927. Please try again in 12m10.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:13,068 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:13,068 - src.methods.zero_shot - ERROR - Failed to process document 17: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4927. Please try again in 12m10.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:13,068 - src.methods.zero_shot - INFO - Processing document 18/32: BRITANNIA (transcript)
2026-01-28 19:57:13,068 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q1 2025 (Zero-shot)
2026-01-28 19:57:13,068 - src.methods.zero_shot - WARNING - Truncating text from 49804 to 20000 chars
2026-01-28 19:57:13,115 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:13,115 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4910. Please try again in 11m56.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:13,116 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95919, Requested 4910. Please try again in 11m56.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:15,173 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:15,174 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95917, Requested 4910. Please try again in 11m54.528s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:15,174 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95917, Requested 4910. Please try again in 11m54.528s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:19,443 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:19,459 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4910. Please try again in 11m50.208s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:19,459 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:19,459 - src.methods.zero_shot - ERROR - Failed to process document 18: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4910. Please try again in 11m50.208s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:19,459 - src.methods.zero_shot - INFO - Processing document 19/32: BRITANNIA (transcript)
2026-01-28 19:57:19,459 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q2 2024 (Zero-shot)
2026-01-28 19:57:19,459 - src.methods.zero_shot - WARNING - Truncating text from 46322 to 20000 chars
2026-01-28 19:57:19,508 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:19,508 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4954. Please try again in 12m28.223999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:19,508 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95912, Requested 4954. Please try again in 12m28.223999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:21,783 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:21,785 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95909, Requested 4954. Please try again in 12m25.632s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:21,785 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95909, Requested 4954. Please try again in 12m25.632s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:25,842 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:25,842 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95904, Requested 4954. Please try again in 12m21.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:25,842 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:25,842 - src.methods.zero_shot - ERROR - Failed to process document 19: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95904, Requested 4954. Please try again in 12m21.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:25,842 - src.methods.zero_shot - INFO - Processing document 20/32: BRITANNIA (transcript)
2026-01-28 19:57:25,842 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q2 2025 (Zero-shot)
2026-01-28 19:57:25,842 - src.methods.zero_shot - WARNING - Truncating text from 45706 to 20000 chars
2026-01-28 19:57:25,892 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:25,924 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95904, Requested 4997. Please try again in 12m58.464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:25,925 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95904, Requested 4997. Please try again in 12m58.464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:27,975 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:27,976 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95902, Requested 4997. Please try again in 12m56.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:27,977 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95902, Requested 4997. Please try again in 12m56.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:32,006 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:32,006 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95897, Requested 4997. Please try again in 12m52.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:32,006 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:32,006 - src.methods.zero_shot - ERROR - Failed to process document 20: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95897, Requested 4997. Please try again in 12m52.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:32,006 - src.methods.zero_shot - INFO - Processing document 21/32: BRITANNIA (transcript)
2026-01-28 19:57:32,021 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q3 2024 (Zero-shot)
2026-01-28 19:57:32,021 - src.methods.zero_shot - WARNING - Truncating text from 49047 to 20000 chars
2026-01-28 19:57:32,044 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:32,044 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95897, Requested 4897. Please try again in 11m26.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:32,055 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95897, Requested 4897. Please try again in 11m26.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:34,100 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:34,101 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95895, Requested 4897. Please try again in 11m24.288s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:34,102 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95895, Requested 4897. Please try again in 11m24.288s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:38,162 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:38,164 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95890, Requested 4897. Please try again in 11m19.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:38,164 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:38,165 - src.methods.zero_shot - ERROR - Failed to process document 21: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95890, Requested 4897. Please try again in 11m19.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:38,165 - src.methods.zero_shot - INFO - Processing document 22/32: BRITANNIA (transcript)
2026-01-28 19:57:38,166 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q3 2025 (Zero-shot)
2026-01-28 19:57:38,166 - src.methods.zero_shot - WARNING - Truncating text from 45521 to 20000 chars
2026-01-28 19:57:38,195 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:38,195 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95890, Requested 5019. Please try again in 13m5.376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:38,195 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95890, Requested 5019. Please try again in 13m5.376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:40,230 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:40,231 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95888, Requested 5019. Please try again in 13m3.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:40,232 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95888, Requested 5019. Please try again in 13m3.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:44,287 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:44,287 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95883, Requested 5019. Please try again in 12m59.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:44,287 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:44,287 - src.methods.zero_shot - ERROR - Failed to process document 22: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95883, Requested 5019. Please try again in 12m59.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:44,287 - src.methods.zero_shot - INFO - Processing document 23/32: BRITANNIA (transcript)
2026-01-28 19:57:44,287 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q4 2024 (Zero-shot)
2026-01-28 19:57:44,287 - src.methods.zero_shot - WARNING - Truncating text from 45461 to 20000 chars
2026-01-28 19:57:44,327 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:44,328 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95883, Requested 4890. Please try again in 11m7.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:44,328 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95883, Requested 4890. Please try again in 11m7.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:46,370 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:46,370 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95880, Requested 4890. Please try again in 11m5.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:46,371 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95880, Requested 4890. Please try again in 11m5.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:50,404 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:50,405 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95876, Requested 4890. Please try again in 11m1.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:50,405 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:50,406 - src.methods.zero_shot - ERROR - Failed to process document 23: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95876, Requested 4890. Please try again in 11m1.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:50,406 - src.methods.zero_shot - INFO - Processing document 24/32: BRITANNIA (transcript)
2026-01-28 19:57:50,407 - src.methods.zero_shot - INFO - Extracting commentary for BRITANNIA Q4 2025 (Zero-shot)
2026-01-28 19:57:50,408 - src.methods.zero_shot - WARNING - Truncating text from 47802 to 20000 chars
2026-01-28 19:57:50,442 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:50,443 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95876, Requested 5035. Please try again in 13m7.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:50,443 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95876, Requested 5035. Please try again in 13m7.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:52,500 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:52,501 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95873, Requested 5035. Please try again in 13m4.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:52,502 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95873, Requested 5035. Please try again in 13m4.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:57:56,564 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:56,565 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95869, Requested 5035. Please try again in 13m1.055999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:56,566 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:57:56,566 - src.methods.zero_shot - ERROR - Failed to process document 24: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95869, Requested 5035. Please try again in 13m1.055999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:56,567 - src.methods.zero_shot - INFO - Processing document 25/32: DABUR (transcript)
2026-01-28 19:57:56,567 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q1 2024 (Zero-shot)
2026-01-28 19:57:56,567 - src.methods.zero_shot - WARNING - Truncating text from 61085 to 20000 chars
2026-01-28 19:57:56,610 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:56,611 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95869, Requested 4908. Please try again in 11m11.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:56,612 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95869, Requested 4908. Please try again in 11m11.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:57:58,906 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:57:58,906 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95866, Requested 4908. Please try again in 11m8.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:57:58,906 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95866, Requested 4908. Please try again in 11m8.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:02,929 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:02,944 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95861, Requested 4908. Please try again in 11m4.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:02,944 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:02,944 - src.methods.zero_shot - ERROR - Failed to process document 25: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95861, Requested 4908. Please try again in 11m4.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:02,944 - src.methods.zero_shot - INFO - Processing document 26/32: DABUR (transcript)
2026-01-28 19:58:02,944 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q1 2025 (Zero-shot)
2026-01-28 19:58:02,944 - src.methods.zero_shot - WARNING - Truncating text from 58416 to 20000 chars
2026-01-28 19:58:03,233 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:03,235 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95861, Requested 4901. Please try again in 10m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:03,236 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95861, Requested 4901. Please try again in 10m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:05,289 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:05,289 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95859, Requested 4901. Please try again in 10m56.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:05,289 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95859, Requested 4901. Please try again in 10m56.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:09,337 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:09,337 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95854, Requested 4901. Please try again in 10m52.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:09,337 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:09,337 - src.methods.zero_shot - ERROR - Failed to process document 26: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95854, Requested 4901. Please try again in 10m52.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:09,337 - src.methods.zero_shot - INFO - Processing document 27/32: DABUR (transcript)
2026-01-28 19:58:09,337 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q2 2024 (Zero-shot)
2026-01-28 19:58:09,337 - src.methods.zero_shot - WARNING - Truncating text from 37834 to 20000 chars
2026-01-28 19:58:09,387 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:09,387 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95854, Requested 4906. Please try again in 10m56.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:09,387 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95854, Requested 4906. Please try again in 10m56.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:11,667 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:11,683 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95851, Requested 4906. Please try again in 10m54.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:11,683 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95851, Requested 4906. Please try again in 10m54.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:15,752 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:15,752 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95846, Requested 4906. Please try again in 10m49.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:15,766 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:15,767 - src.methods.zero_shot - ERROR - Failed to process document 27: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95846, Requested 4906. Please try again in 10m49.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:15,768 - src.methods.zero_shot - INFO - Processing document 28/32: DABUR (transcript)
2026-01-28 19:58:15,768 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q2 2025 (Zero-shot)
2026-01-28 19:58:15,769 - src.methods.zero_shot - WARNING - Truncating text from 68411 to 20000 chars
2026-01-28 19:58:15,807 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:15,833 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95846, Requested 4863. Please try again in 10m12.576s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:15,835 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95846, Requested 4863. Please try again in 10m12.576s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:17,890 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:17,891 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95844, Requested 4863. Please try again in 10m10.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:17,892 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95844, Requested 4863. Please try again in 10m10.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:22,193 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:22,193 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95839, Requested 4863. Please try again in 10m6.528s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:22,193 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:22,193 - src.methods.zero_shot - ERROR - Failed to process document 28: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95839, Requested 4863. Please try again in 10m6.528s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:22,193 - src.methods.zero_shot - INFO - Processing document 29/32: DABUR (transcript)
2026-01-28 19:58:22,193 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q3 2024 (Zero-shot)
2026-01-28 19:58:22,193 - src.methods.zero_shot - WARNING - Truncating text from 51644 to 20000 chars
2026-01-28 19:58:22,229 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:22,230 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95839, Requested 4909. Please try again in 10m46.271999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:22,231 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95839, Requested 4909. Please try again in 10m46.271999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:24,279 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:24,280 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95837, Requested 4909. Please try again in 10m44.544s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:24,281 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95837, Requested 4909. Please try again in 10m44.544s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:28,334 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:28,334 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95832, Requested 4909. Please try again in 10m40.224s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:28,334 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:28,350 - src.methods.zero_shot - ERROR - Failed to process document 29: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95832, Requested 4909. Please try again in 10m40.224s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:28,350 - src.methods.zero_shot - INFO - Processing document 30/32: DABUR (transcript)
2026-01-28 19:58:28,350 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q3 2025 (Zero-shot)
2026-01-28 19:58:28,350 - src.methods.zero_shot - WARNING - Truncating text from 62347 to 20000 chars
2026-01-28 19:58:28,383 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:28,383 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95832, Requested 4966. Please try again in 11m29.472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:28,383 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95832, Requested 4966. Please try again in 11m29.472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:30,413 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:30,413 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95830, Requested 4966. Please try again in 11m27.744s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:30,413 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95830, Requested 4966. Please try again in 11m27.744s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:34,459 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:34,459 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95825, Requested 4966. Please try again in 11m23.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:34,459 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:34,459 - src.methods.zero_shot - ERROR - Failed to process document 30: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95825, Requested 4966. Please try again in 11m23.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:34,459 - src.methods.zero_shot - INFO - Processing document 31/32: DABUR (transcript)
2026-01-28 19:58:34,459 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q4 2024 (Zero-shot)
2026-01-28 19:58:34,459 - src.methods.zero_shot - WARNING - Truncating text from 74312 to 20000 chars
2026-01-28 19:58:34,491 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:34,491 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95825, Requested 4940. Please try again in 11m0.959999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:34,491 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95825, Requested 4940. Please try again in 11m0.959999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:36,533 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:36,534 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95822, Requested 4940. Please try again in 10m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:36,535 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95822, Requested 4940. Please try again in 10m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:40,581 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:40,581 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95818, Requested 4940. Please try again in 10m54.912s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:40,581 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:40,581 - src.methods.zero_shot - ERROR - Failed to process document 31: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95818, Requested 4940. Please try again in 10m54.912s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:40,581 - src.methods.zero_shot - INFO - Processing document 32/32: DABUR (transcript)
2026-01-28 19:58:40,581 - src.methods.zero_shot - INFO - Extracting commentary for DABUR Q4 2025 (Zero-shot)
2026-01-28 19:58:40,581 - src.methods.zero_shot - WARNING - Truncating text from 46453 to 20000 chars
2026-01-28 19:58:40,864 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:40,864 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95818, Requested 4935. Please try again in 10m50.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:40,864 - src.utils.helpers - WARNING - _call_llm failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95818, Requested 4935. Please try again in 10m50.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 2.0s...
2026-01-28 19:58:43,187 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:43,188 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95815, Requested 4935. Please try again in 10m48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:43,188 - src.utils.helpers - WARNING - _call_llm failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95815, Requested 4935. Please try again in 10m48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Retrying in 4.0s...
2026-01-28 19:58:47,229 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2026-01-28 19:58:47,230 - src.methods.zero_shot - ERROR - LLM API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95810, Requested 4935. Please try again in 10m43.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:47,230 - src.utils.helpers - ERROR - _call_llm failed after 3 attempts
2026-01-28 19:58:47,231 - src.methods.zero_shot - ERROR - Failed to process document 32: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k7bhxxgqfh6vkxp73jnma9va` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 95810, Requested 4935. Please try again in 10m43.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2026-01-28 19:58:47,298 - src.methods.zero_shot - INFO - Batch extraction complete. Total cost: $0.0576
2026-01-28 19:58:47,299 - experiment.financial_rag_pipeline - INFO - Metrics:
2026-01-28 19:58:47,300 - experiment.financial_rag_pipeline - INFO -   zero_shot_model: llama-3.3-70b-versatile
2026-01-28 19:58:47,300 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_cost: 0.05764563000000002
2026-01-28 19:58:47,301 - experiment.financial_rag_pipeline - INFO -   zero_shot_total_tokens: 96417
2026-01-28 19:58:47,301 - experiment.financial_rag_pipeline - INFO -   zero_shot_num_requests: 14
2026-01-28 19:58:47,302 - src.utils.helpers - INFO - run_zero_shot_extraction took 541.73 seconds

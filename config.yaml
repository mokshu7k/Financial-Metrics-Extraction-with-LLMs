project:
  name: "Financial RAG Research"
  version: "1.0.0"
  description: "Research pipeline for financial document processing and RAG evaluation"

# Data paths configuration
paths:
  data_root: "data"
  raw_data: "data/raw"
  processed_data: "data/processed"
  ground_truth: "data/ground_truth"
  embeddings: "data/embeddings"
  results: "results"
  logs: "results/logs"

# Document processing configuration
preprocessing:
  # Text cleaning parameters
  cleaning:
    remove_headers_footers: true
    normalize_whitespace: true
    remove_page_numbers: true
    normalize_currency: true
    
  # Chunking parameters
  chunking:
    default_strategy: "fixed_size"  # Options: fixed_size, paragraph, section, semantic
    chunk_size: 1000  # tokens
    chunk_overlap: 200  # tokens
    min_chunk_size: 100  # tokens
    max_chunk_size: 2000  # tokens
    
    # Strategy-specific settings
    strategies:
      fixed_size:
        size: 1000
        overlap: 200
      paragraph:
        max_paragraph_length: 1500
        merge_short_paragraphs: true
      section:
        preserve_section_boundaries: true
        min_section_length: 200
      semantic:
        similarity_threshold: 0.7
        max_semantic_chunk_size: 1200

# Embedding configuration
embeddings:
  # Model settings
  model:
    name: "all-MiniLM-L6-v2"  # Options: all-MiniLM-L6-v2, all-mpnet-base-v2, etc.
    dimension: 384
    normalize: true
    batch_size: 32
    device: "cpu"  # Options: cpu, cuda
  
  # Alternative models for comparison
  alternative_models:
    - name: "all-mpnet-base-v2"
      dimension: 768
    - name: "sentence-transformers/all-MiniLM-L12-v2"
      dimension: 384

# Vector store configuration
vector_store:
  backend: "faiss"  # Options: faiss, chroma, pinecone
  index_type: "flat"  # Options: flat, ivf, hnsw
  similarity_metric: "cosine"  # Options: cosine, dot, euclidean
  
  # FAISS specific settings
  faiss:
    index_factory: "Flat"
    nprobe: 10  # for IVF indices
    
  # Chroma specific settings
  chroma:
    collection_name: "financial_chunks"
    persist_directory: "data/embeddings/chroma"
    
  # Search parameters
  search:
    default_k: 10
    max_k: 50
    score_threshold: 0.0

# RAG methodology configuration
rag_methods:
  # Zero-shot baseline
  zero_shot:
    enabled: true
    model: "llama-3.3-70b-versatile"  # or claude-3-sonnet-20240229
    temperature: 0.1
    max_tokens: 1000
    
  # Chain of thought
  chain_of_thought:
    enabled: true
    model: "llama-3.3-70b-versatile"  # or claude-3-opus-20240229
    temperature: 0.1
    max_tokens: 1500
    reasoning_steps: 3
    
  # RAG pipeline
  rag:
    enabled: true
    retrieval_k: 5
    reranking: true
    model: "llama-3.3-70b-versatile"
    temperature: 0.1
    max_tokens: 2000
    context_window: 4000
    
    # Retrieval settings
    retrieval:
      semantic_search: true
      keyword_boost: 0.2
      recency_boost: 0.1
      company_filter: true
      
    # Reranking settings
    reranking:
      model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      top_k: 10

# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics:
    accuracy:
      - exact_match
      - f1_score
      - bleu
      - rouge_l
    efficiency:
      - response_time
      - token_usage
      - cost_per_query
      - throughput
    retrieval:
      - precision_at_k: [1, 5, 10]
      - recall_at_k: [1, 5, 10]
      - mrr
      - ndcg
  
  # Test set configuration
  test_set:
    size: 100
    split_ratio: 0.8  # train/test split
    stratify_by: "company"
    
  # Ground truth annotation
  annotation:
    annotators: 2
    agreement_threshold: 0.8
    annotation_guidelines: "data/ground_truth/guidelines.md"

# Experimental setup
experiments:
  # Comparison studies
  studies:
    - name: "method_comparison"
      description: "Compare zero-shot, CoT, and RAG approaches"
      methods: ["zero_shot", "chain_of_thought", "rag"]
      
    - name: "chunking_strategies"
      description: "Evaluate different chunking approaches"
      chunking_methods: ["fixed_size", "paragraph", "section"]
      
    - name: "embedding_models"
      description: "Compare embedding model performance"
      models: ["all-MiniLM-L6-v2", "all-mpnet-base-v2"]
      
    - name: "retrieval_tuning"
      description: "Optimize retrieval parameters"
      parameters:
        k_values: [3, 5, 10, 15]
        reranking: [true, false]
  
  # Output configuration
  output:
    save_intermediate: true
    save_predictions: true
    save_retrieval_results: true
    create_visualizations: true

# Logging configuration
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "results/logs/financial_rag.log"
  console: true
  
  # Component-specific logging
  components:
    preprocessing: "INFO"
    embedding: "INFO"
    retrieval: "DEBUG"
    evaluation: "INFO"

# Resource limits
resources:
  max_memory_gb: 8
  max_workers: 4
  timeout_minutes: 30
  cache_size_mb: 1000

# API configurations (if using external APIs)
apis:
  openai:
    api_key_env: "OPENAI_API_KEY"
    rate_limit: 60  # requests per minute
    retry_attempts: 3
    
  anthropic:
    api_key_env: "ANTHROPIC_API_KEY"
    rate_limit: 40  # requests per minute
    retry_attempts: 3

# Development and debugging
development:
  debug_mode: false
  sample_size: 50  # Use smaller dataset for testing
  skip_embedding: false  # Skip embedding generation in debug mode
  verbose_output: true
  save_intermediate_results: true
